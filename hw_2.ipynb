{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2, Intro to NLP, 2017\n",
    "\n",
    "#### This is due at 11pm on Tuesday, October 10. Please see detailed submission instructions below.  100 points total.\n",
    "\n",
    "##### How to do this problem set:\n",
    "\n",
    "- What version of Python should I use? 2.7\n",
    "\n",
    "- Most of these questions require writing Python code and computing results, and the rest of them have textual answers. To generate the answers, you will have to fill out supporting files, `vit_starter.py`,`classperc.py` and `structperc.py`.\n",
    "\n",
    "- Write all the answers in this ipython notebook. Once you are finished (1) Generate a PDF via (File -> Download As -> PDF) and upload to Gradescope (2)Turn in `vit_starter.py`, `classperc.py`, `structperc.py` and `hw_2.ipynb` on Moodle.\n",
    "  \n",
    "- **Important:** Check your PDF before you turn it in to gradescope to make sure it exported correctly. If ipython notebook gets confused about your syntax it will sometimes terminate the PDF creation routine early. You are responsible for checking for these errors. If your whole PDF does not print, try running `$jupyter nbconvert --to pdf hw_1.ipynb` to identify and fix any syntax errors that might be causing problems.\n",
    "\n",
    "- **Important:** When creating your final version of the PDF to hand in, please do a fresh restart and execute every cell in order. Then you'll be sure it's actually right. One convenient way to do this is by clicking `Cell -> Run All` in the notebook menu.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Academic honesty \n",
    "\n",
    "- We will audit the Moodle code from a few dozen students, chosen at random. The audits will check that the code you wrote and turned on Moodle generates the answers you turn in on your Gradescope PDF. If you turn in correct answers on your PDF without code that actually generates those answers, we will consider this a potential case of cheating. See the course page for honesty policies.\n",
    "\n",
    "- We will also run automatic checks of code on Moodle for plagiarism. Copying code from others is considered a serious case of cheating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. HMM (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions using the transition matrix\n",
    "$T$ and emission probabilities $E$ below. Below, $\\Delta$ and $\\Box$\n",
    "are two output variables, $A$ and $B$ are two hidden states; $s_n$ refers to\n",
    "the $n^{th}$ hidden state in the sequence and $o_n$ refers to the\n",
    "$n^{th}$ observation.\n",
    "\n",
    "<img src=\"2.png\"> \n",
    "\n",
    "For all the questions in this section, write answer and show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1 (2 points)**\n",
    "\n",
    "Does $P(o_2=\\Delta|s_1=B) = P(o_2=\\Delta|o_1=\\Box)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " The seq of states can be BA OR BB.                                          \n",
    "  For $P(o_2=\\Delta|s_1=B)$ =$P(o_2=\\Delta|s_2 =B)$ * $ P(s_2=B) $ \n",
    "  $+ P(o_2=\\Delta|s_2=A)$ * $ P(s_2=A) $  =\n",
    "  $ (0.3* 0.4 )+ (0.5*0.4) $ =0.32      \n",
    "  \n",
    " $P(o_2=\\Delta|o_1=\\Box) $=$P(o_2=\\Delta,s_2=A/s_1=A,o_1=\\Box) $\n",
    " \n",
    " \n",
    "$* P(o_2=\\Delta,s_2=B/s_1=A,o_1=\\Box) * P(o_2=\\Delta,s_2=B/s_1=B,o_1=\\Box) * $\n",
    "\n",
    "$ P(o_2=\\Delta,s_2=A/s_1=B,o_1=\\Box) =$\n",
    "\n",
    "$ 0.5*0.2*0.5 + 0.5*0.3*0.3 +0.3*0.7*0.4+0.5*0.3*0.4 =0.24 $\n",
    " \n",
    "Hence No."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2 (2 points)**\n",
    "\n",
    "Does $P(s_2=B|s_1=A) = P(s_2=B|s_1=A, o_1 = \\Delta)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "No. the emission matrix does affect with the transition of states.\n",
    "\n",
    "$ P(s_2=B|s_1=A)=0.3  $    \n",
    "\n",
    "$P(s_2=B|s_1=A, o_1 = \\Delta)=0.5*0.3=0.15 $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.3 (3 points)**\n",
    "\n",
    "Does $P(o_2=\\Delta|s_1=A) = P(o_2=\\Box|s_1=A, s_3=A)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "No.\n",
    "\n",
    "$P(o_2=\\Delta|s_1=A)= 0.2*0.5+0.3*0.3 =0.19 $\n",
    "\n",
    "$P(o_2=\\Box|s_1=A, s_3=A)= 0.2*0.5*0.2+0.3*0.3*0.4=0.056 $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.4 (3 points)**\n",
    "\n",
    "Compute the probability of observing $\\Box$ as the first emission of a sequence generated by an HMM with transition\n",
    "matrix $T$ and emission probabilities $E$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "$P(o_1=\\Box)=P(o_1=\\Box|s_1=B)+P(o_1=\\Box|s_1=A) =0.5*0.5+ 0.5*0.7 =0.60 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.5 (5 points)**\n",
    "\n",
    "Compute the probability of the first state being $A$ given that the last token in an observed sequence of length 2\n",
    "was the token $\\Delta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$ P(  s_1=A | o_2=\\Delta) = P(A) * P(s_2=A|s_1=A)* P(o_2=\\Delta | s_2=A) + $\n",
    "\n",
    "$ P(A) * P(s_2=B|s_1=A) *P(o_2=\\Delta | s_2=B) =$\n",
    "$ 0.5*0.5*0.5 + 0.5*0.3*0.3 = 0.125 $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Viterbi (log-additive form) (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1.png\"> \n",
    "\n",
    "\n",
    "\n",
    "One HMM chain is shown on the left.  The corresponding **factor graph** version is shown on the right.\n",
    "This simply shows the structure of the $A$ and $B_t$ log-prob tables and which variables they express\n",
    "preferences over. $A$ is the **transition factor** that has preferences for the two neighboring\n",
    "variables; for example, $A(y_1,y_2)$ shows how happy the model is with the transition from $y_1$ to $y_2$.\n",
    "The same transition preference function is used at all positions $(t-1,t)$ for each $t=2..T$.\n",
    "$B_t$ is the **emission factor** that has preferences for the variable $y_t$.\n",
    "As a goodness function it is e.g. $B_1(y_1)$, $B_2(y_2)$, etc.\n",
    "\n",
    "Let $\\vec{y} = (y_1,y_2,...,y_T)$, a proposed tag sequence for a $T$ length sentence.\n",
    "The total goodness function for a solution $\\vec{y}$ is\n",
    "\n",
    "$$ G(\\vec{y}) = \\sum_{t=1}^{T} B_t(y_t)  + \\sum_{t=2}^{T} A(y_{t-1},y_t) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1 (2 points)**\n",
    "\n",
    "Define $A$ and $B_t$ in terms of the HMM model, such that $G$ is the same thing as $\\log p(\\vec{y},\\vec{w})$ under the HMM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Write the mathematical expressions here.**\n",
    "\n",
    "$ A(y_1,y_2) =log P_ t(y_2,y_1)  $\n",
    "\n",
    "$ B(y_1)=log_e(w_1|y_1) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2 (18 points)**\n",
    "\n",
    "Implement additive log-space Viterbi by completing the **viterbi()** function. It takes in tables that represent the $A$ and $B$ functions as input.  We give you an implementation of $G()$ in **vit_starter**, you can check to make sure you understand the data structures, and also the exhaustive decoding algorithm too.  Feel free to add debugging print statements as needed.  The main code runs the exercise example by default.\n",
    "\n",
    "When debugging, you should make new A and B examples that are very simple. This will test different code paths.  Also you can try the **randomized\\_test()** from the starter code.\n",
    "\n",
    "Look out for negative indexes as a bug.  In python, if you use an index that's too high to be in the list, it throws an error.  But it will silently accept a negative index ... it interprets that as indexing from the right.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exhaustive decoding: [0, 0, 0]\n",
      "score: 36\n",
      "Viterbi    decoding: [0, 0, 0]\n",
      "output_vocab= [0, 1, 2, 3, 4]\n",
      "A= {(1, 3): 0.6851245350951041, (3, 0): 0.42215178560308453, (2, 1): 0.23929926042326466, (0, 3): 0.6982263616973808, (4, 0): 0.3137629890368333, (1, 2): 0.43379350874544576, (3, 3): 0.2984973499311572, (4, 4): 0.831330962405385, (2, 2): 0.4465417443130443, (4, 1): 0.977830205112694, (1, 1): 0.9480266772852576, (3, 2): 0.9493722670027007, (0, 0): 0.23303541964148966, (0, 4): 0.2677109930792967, (1, 4): 0.8192806604626953, (2, 3): 0.9123162279776676, (4, 2): 0.6799478370539035, (1, 0): 0.05204418503625696, (0, 1): 0.40351649245171994, (3, 1): 0.588970286938972, (2, 4): 0.32669869716518574, (2, 0): 0.907526351918115, (4, 3): 0.835172276717493, (3, 4): 0.9194467727345205, (0, 2): 0.8492467130441527}\n",
      "Bs= [[0.5094892751572917, 0.6827659891756795, 0.9070355097729869, 4.508144865977837e-05, 0.06132531347103509], [0.2913242544847646, 0.07758390759601619, 0.35287525232225814, 0.670926115164696, 0.2638218531150238], [0.4160762655503695, 0.7954392679800923, 0.7342160276508459, 0.2871379964956675, 0.020748778586912153]]\n",
      "Worked!\n"
     ]
    }
   ],
   "source": [
    "# Implement the viterbi() function in vit_starter.py and then run this cell to show your output\n",
    "\n",
    "from vit_starter import *\n",
    "\n",
    "if __name__=='__main__':\n",
    "    A = {(0,0):3, (0,1):0, (1,0):0, (1,1):3}\n",
    "    Bs= [ [0,1], [0,1], [30,0] ]\n",
    "    # that's equivalent to: [ {0:0,1:1}, {0:0,1:1}, {0:30,1:0} ]\n",
    "\n",
    "    y = exhaustive(A, Bs, set([0,1]))\n",
    "    print \"Exhaustive decoding:\", y\n",
    "    print \"score:\", goodness_score(y, A, Bs)\n",
    "    y= viterbi(A, Bs, set([0,1]))\n",
    "    print \"Viterbi    decoding:\", y\n",
    "    randomized_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Copy and paste the viterbi function that you implemented in `vit_starter.py`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi(A_factor, B_factors, output_vocab):\n",
    "    \"\"\"\n",
    "    A_factor: a dict of key:value pairs of the form\n",
    "        {(curtag,nexttag): score}\n",
    "    with keys for all K^2 possible neighboring combinations,\n",
    "    and scores are numbers.  We assume they should be used ADDITIVELY, i.e. in log space.\n",
    "    higher scores mean MORE PREFERRED by the model.\n",
    "\n",
    "    B_factors: a list where each entry is a dict {tag:score}, so like\n",
    "    [ {Noun:-1.2, Adj:-3.4}, {Noun:-0.2, Adj:-7.1}, .... ]\n",
    "    each entry in the list corresponds to each position in the input.\n",
    "\n",
    "    output_vocab: a set of strings, which is the vocabulary of possible output\n",
    "    symbols.\n",
    "\n",
    "    RETURNS:\n",
    "    the tag sequence yvec with the highest goodness score\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(B_factors)   # length of input sentence\n",
    "\n",
    "    # viterbi log-prob tables\n",
    "    V = [{tag:None for tag in output_vocab} for t in range(N)]\n",
    "    # backpointer tables\n",
    "    # back[0] could be left empty. it will never be used.\n",
    "    back = [{tag:None for tag in output_vocab} for t in range(N)]\n",
    "\n",
    "    # todo implement the main viterbi loop here\n",
    "    # you may want to handle the t=0 case separately\n",
    "    V[0][0]=B_factors[0][0]\n",
    "    V[0][1]=B_factors[0][1] # for t =1\n",
    "    xx=[]\n",
    "    \n",
    "    \n",
    "    # todo implement backtrace also\n",
    "    for t in range(1,N):\n",
    "        for k in output_vocab:\n",
    "            xx=[]\n",
    "            for j in output_vocab:\n",
    "                xc=V[t-1][j]+A_factor[j,k]+B_factors[t][k]\n",
    "                \n",
    "                xx.append(xc)\n",
    "            V[t][k]=max(xx)\n",
    "           \n",
    "            back[t][k]=xx.index(max(xx))\n",
    "            \n",
    "    val = []\n",
    "    for x,y in V[N-1].items():\n",
    "        val.append(y)\n",
    "    lastVmax = np.argmax(val)\n",
    "    a = list([lastVmax])\n",
    "    for t in list(reversed(range (1,N))):\n",
    "        lastVmax = back[t][lastVmax]\n",
    "        a = list([lastVmax]) + a\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Averaged Perceptron (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the following definition of the perceptron, which is\n",
    "the multiclass or structured version of the perceptron.\n",
    "The training set is a bunch of input-output pairs $(x_i,y_i)$.\n",
    "(For classification, $y_i$ is a label, but for tagging, $y_i$ is a sequence). The training algorithm is as follows:\n",
    "\n",
    "For T iterations, iterate through each $(x_i,y_i)$ pair in the dataset, and for each,\n",
    "   1. Predict $y^* := \\arg\\max_{y'} \\theta^T f(x_i, y')$\n",
    "   2. If $y_i \\neq y^*$: then update $\\theta := \\theta^{(old)} + r g$\n",
    "\n",
    "\n",
    "where $r$ is a fixed step size (e.g. $r=1$)\n",
    "and $g$ is the *gradient vector*, meaning a vector that will get added\n",
    "into $\\theta$ for the update, specifically\n",
    "$$ g = \\underbrace{f(x_i,y_i)}_{\\text{feats of true output}} -  \\underbrace{f(x_i,y^*)}_{\\text{feats of predicted output}} $$\n",
    "\n",
    "Both in theory and in practice, the predictive accuracy of a model trained by the structured perceptron will be better if we use the average value of $\\theta$ over the course of training, rather than the final value of $\\theta$. This is because $\\theta$ wanders around and doesn't converge (typically), because it overfits to whatever data it saw most recently.\n",
    "After seeing $t$ training examples, define the *averaged parameter vector* as\n",
    "<img src=\"4.png\"> \n",
    "\n",
    "where $\\theta_{t'}$ is the weight vector after $t'$ updates.  (We are counting $t$ by the number of training examples, not passes through the data.  So if you had 1000 examples and made 10 passes through the data in order, the final time you see the final example is $t=10000$.)\n",
    "For training, you still use the current $\\theta$ parameter for predictions.\n",
    "But at the very end,\n",
    "you return the $\\bar{\\theta}$, not $\\theta$, as your final model parameters to use on test data.\n",
    "\n",
    "Directly implementing equation (1) would be really slow.  So here's a better algorithm.  This is the same as in Hal Daume's CIML chapter on perceptrons, but adapted for the structured case (as opposed to Daume's algorithm, which assumes binary output).  Define $g_t$ to be the update vector $g$ as described earlier. The perceptron update can be written\n",
    "$$ \\theta_t = \\theta_{t-1} + r g_t $$\n",
    "\n",
    "Thus the averaged perceptron algorithm is, using a new 'weightsums' vector $S$,\n",
    "\n",
    " 1. Initialize $t=1, \\theta_0=\\vec{0}, S_0=\\vec{0}$\n",
    " 2. For each example $i$ (iterating multiples times through dataset),\n",
    "   - Predict $y^* = \\arg\\max_{y'} \\theta^T f(x_i, y')$\n",
    "   - Let $g_t = f(x_i, y_i)-f(x_i, y^*)$\n",
    "   - Update $\\theta_t = \\theta_{t-1} + r g_t$\n",
    "   - Update $S_t = S_{t-1} + (t-1) r g_t$\n",
    "   - $t := t+1$\n",
    "   \n",
    " 3. Calculate $\\bar{\\theta}$ based on $S$\n",
    "\n",
    "\n",
    "In an actual implementation, you don't keep old versions of $S$ or $\\theta$ around ...\n",
    "above we're using the $t$ subscripts above just to make the mathematical analysis clearer.\n",
    "\n",
    "Our proposed algorithm computes $\\bar{\\theta}_t$ as\n",
    "<img src=\"5.png\"> \n",
    "\n",
    "For the following problems, feel free to set $r=1$ just to simplify them.\n",
    "\n",
    "For following questions write only math answers, no code required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1** (1 point)\n",
    "\n",
    "What is the computational advantage\n",
    "of computing $\\bar{\\theta}$ using Equation (2)\n",
    "instead of\n",
    "directly implementing\n",
    "Equation (1)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As there is no need to store intermediate values ,we can save on memory being used .\n",
    "Also we can save on computational time as it is much faster using Eqn (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll show this works, at least for early iterations.\n",
    "\n",
    "**Question 3.2** (1 point)\n",
    "\n",
    "\n",
    "What are $\\bar{\\theta}_1$, $\\bar{\\theta}_2$, $\\bar{\\theta}_3$, and $\\bar{\\theta}_4$?\n",
    "Please derive them from \n",
    "the Equation (1) definition,\n",
    "and state them in terms of $g_1$, $g_2$, $g_3$, and/or $g_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Assuming r =1\n",
    "\n",
    "$\\theta_1$ = $\\theta_0$ + r$g_1$ = $g_1$\n",
    "\n",
    "$\\theta_2$ = $\\theta_1$ + r$g_2$ = $g_1$ + $g_2$\n",
    "\n",
    "$\\theta_3$ = $\\theta_2$ + r$g_3$ = $g_1$ + $g_2$ + $g_3$\n",
    "\n",
    "$\\theta_4$ = $\\theta_3$ + r$g_4$ = $g_1$ + $g_2$ + $g_3$ + $g_4$\n",
    "\n",
    "$\\bar{\\theta}_1$ = $\\theta_1$ = $g_1$\n",
    "\n",
    "$\\bar{\\theta}_2$ = $\\frac{\\theta_1 + \\theta_2}{2}$ = $\\frac{2*g_1 + g_2}{2}$\n",
    "\n",
    "$\\bar{\\theta}_3$ = $\\frac{\\theta_1 + \\theta_2 + \\theta_3}{3}$ = $\\frac{3*g_1 + 2*g_2 + g_3}{3}$\n",
    "\n",
    "$\\bar{\\theta}_4$ = $\\frac{\\theta_1 + \\theta_2 + \\theta_3 + \\theta_4}{4}$ = $\\frac{4*g_1 + 3*g_2 + 2g_3 + g_4}{4}$\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.3** (1 point)\n",
    "\n",
    "What are $S_1$, $S_2$, $S_3$, and $S_4$?\n",
    "Please state them in terms of $g_1$, $g_2$, $g_3$, and/or $g_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For the following, r = 1 is being assumed.\n",
    "\n",
    "$S_0$ = 0\n",
    "\n",
    "$S_1$ = $S_0$ + (1-1)r$g_1$ = 0\n",
    "\n",
    "$S_2$ = $S_1$ + (2-1)r$g_2$ = r$g_2$ = $g_2$\n",
    "\n",
    "$S_3$ = $S_2$ + (3-1)r$g_3$ = r$g_2$ + 2r$g_3$ = $g_2$ + 2$g_3$\n",
    "\n",
    "$S_4$ = $S_3$ + (4-1)r$g_4$ = r$g_2$ + 2r$g_3$ + 3r$g_4$ = $g_2$ + 2$g_3$ + 3$g_4$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.4** (2 points)\n",
    "\n",
    "Show that Equation (2) correctly computes $\\bar{\\theta}_3$ and $\\bar{\\theta}_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$θ_1=rg_1-0 =rg_1$\n",
    "$θ_2=rg_1+rg_2-rg_2/2=rg_1+rg_2/2$\n",
    "$θ_3=rg_1+rg_2+rg_3-rg_2/3-2rg_3/3=rg_1+2rg_2/3+rg_3/3$\n",
    "$θ_4=rg_1+rg_2+rg_3+rg_4-rg_2/4-2rg_3/4-3rg_4/4=rg_1+3rg_2/4+2rg_3/4+rg_4/4$\n",
    "\n",
    "\n",
    "$θ_3,θ¯_4 are the same in 3.1 so it correctly computes$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.5** (2 Extra Credit points)\n",
    "\n",
    "Use proof by induction\n",
    "to show that this algorithm correctly computes $\\bar{\\theta}_t$ for any $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classifier Perceptron (20 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the averaged perceptron for document classification, using the same\n",
    "sentiment analysis dataset as you used for HW1.\n",
    "On the first two questions, we're asking you to develop using only a subset of the data,\n",
    "since that makes debugging easier. On the third question, you'll run\n",
    "on the full dataset, and \n",
    "you should be able to achieve a higher accuracy compared to your previous\n",
    "Naive Bayes implementation.\n",
    "Starter code is provided in `classperc.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "classperc.py:303: SyntaxWarning: import * only allowed at module level\n",
      "  def plot_accuracy_vs_iteration(train_acc, test_acc, avg_test_acc, naive_bayes_acc):\n"
     ]
    }
   ],
   "source": [
    "import classperc as f1\n",
    "from classperc import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[constructing dataset...]\n",
      "\treading data from large_movie_review_dataset\\train\\pos\n",
      "\treading data from large_movie_review_dataset\\train\\neg\n",
      "[dataset constructed.]\n",
      "[constructing dataset...]\n",
      "\treading data from large_movie_review_dataset\\test\\pos\n",
      "\treading data from large_movie_review_dataset\\test\\neg\n",
      "[dataset constructed.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dtr=construct_dataset()\n",
    "dte=construct_dataset(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.1** (8 points) \n",
    "\n",
    "Implement the simple, non-averaged perceptron.  Run your\n",
    "code on **the first 1000 training instances** for 10\n",
    "  passes through the training data. For each pass, report **the\n",
    "  training and test set accuracies**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[training...]\n",
      "\tTraining iteration 0\n",
      "TR RAW EVAL: 1350/2000 = 0.6750 accuracy\n",
      "DEV RAW EVAL: 1249/2000 = 0.6245 accuracy\n",
      "\tTraining iteration 1\n",
      "TR RAW EVAL: 1765/2000 = 0.8825 accuracy\n",
      "DEV RAW EVAL: 1556/2000 = 0.7780 accuracy\n",
      "\tTraining iteration 2\n",
      "TR RAW EVAL: 1675/2000 = 0.8375 accuracy\n",
      "DEV RAW EVAL: 1425/2000 = 0.7125 accuracy\n",
      "\tTraining iteration 3\n",
      "TR RAW EVAL: 1842/2000 = 0.9210 accuracy\n",
      "DEV RAW EVAL: 1549/2000 = 0.7745 accuracy\n",
      "\tTraining iteration 4\n",
      "TR RAW EVAL: 1905/2000 = 0.9525 accuracy\n",
      "DEV RAW EVAL: 1584/2000 = 0.7920 accuracy\n",
      "\tTraining iteration 5\n",
      "TR RAW EVAL: 1842/2000 = 0.9210 accuracy\n",
      "DEV RAW EVAL: 1488/2000 = 0.7440 accuracy\n",
      "\tTraining iteration 6\n",
      "TR RAW EVAL: 1948/2000 = 0.9740 accuracy\n",
      "DEV RAW EVAL: 1579/2000 = 0.7895 accuracy\n",
      "\tTraining iteration 7\n",
      "TR RAW EVAL: 1962/2000 = 0.9810 accuracy\n",
      "DEV RAW EVAL: 1561/2000 = 0.7805 accuracy\n",
      "\tTraining iteration 8\n",
      "TR RAW EVAL: 1928/2000 = 0.9640 accuracy\n",
      "DEV RAW EVAL: 1527/2000 = 0.7635 accuracy\n",
      "\tTraining iteration 9\n",
      "TR RAW EVAL: 1965/2000 = 0.9825 accuracy\n",
      "DEV RAW EVAL: 1594/2000 = 0.7970 accuracy\n",
      "[learned weights for 78254 features from 2000 examples.]\n"
     ]
    }
   ],
   "source": [
    "x=train(dtr,devdata=dte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.2** (8 points)\n",
    "\n",
    "Implement the averaged perceptron. Run your code on\n",
    "**the first 1000 training instances** for 10 passes\n",
    "  through the training data. For each pass, compute the\n",
    "$\\bar{\\theta}$ so far, and report its **test set accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[training...]\n",
      "\tTraining iteration 0\n",
      "TR RAW EVAL: 1606/2000 = 0.8030 accuracy\n",
      "DEV RAW EVAL: 1459/2000 = 0.7295 accuracy\n",
      "DEV AVG EVAL: 1469/2000 = 0.7345 accuracy\n",
      "\tTraining iteration 1\n",
      "TR RAW EVAL: 1607/2000 = 0.8035 accuracy\n",
      "DEV RAW EVAL: 1428/2000 = 0.7140 accuracy\n",
      "DEV AVG EVAL: 1447/2000 = 0.7235 accuracy\n",
      "\tTraining iteration 2\n",
      "TR RAW EVAL: 1834/2000 = 0.9170 accuracy\n",
      "DEV RAW EVAL: 1544/2000 = 0.7720 accuracy\n",
      "DEV AVG EVAL: 1573/2000 = 0.7865 accuracy\n",
      "\tTraining iteration 3\n",
      "TR RAW EVAL: 1757/2000 = 0.8785 accuracy\n",
      "DEV RAW EVAL: 1466/2000 = 0.7330 accuracy\n",
      "DEV AVG EVAL: 1503/2000 = 0.7515 accuracy\n",
      "\tTraining iteration 4\n",
      "TR RAW EVAL: 1917/2000 = 0.9585 accuracy\n",
      "DEV RAW EVAL: 1553/2000 = 0.7765 accuracy\n",
      "DEV AVG EVAL: 1558/2000 = 0.7790 accuracy\n",
      "\tTraining iteration 5\n",
      "TR RAW EVAL: 1917/2000 = 0.9585 accuracy\n",
      "DEV RAW EVAL: 1557/2000 = 0.7785 accuracy\n",
      "DEV AVG EVAL: 1562/2000 = 0.7810 accuracy\n",
      "\tTraining iteration 6\n",
      "TR RAW EVAL: 1936/2000 = 0.9680 accuracy\n",
      "DEV RAW EVAL: 1574/2000 = 0.7870 accuracy\n",
      "DEV AVG EVAL: 1571/2000 = 0.7855 accuracy\n",
      "\tTraining iteration 7\n",
      "TR RAW EVAL: 1940/2000 = 0.9700 accuracy\n",
      "DEV RAW EVAL: 1556/2000 = 0.7780 accuracy\n",
      "DEV AVG EVAL: 1551/2000 = 0.7755 accuracy\n",
      "\tTraining iteration 8\n",
      "TR RAW EVAL: 1902/2000 = 0.9510 accuracy\n",
      "DEV RAW EVAL: 1540/2000 = 0.7700 accuracy\n",
      "DEV AVG EVAL: 1570/2000 = 0.7850 accuracy\n",
      "\tTraining iteration 9\n",
      "TR RAW EVAL: 1959/2000 = 0.9795 accuracy\n",
      "DEV RAW EVAL: 1609/2000 = 0.8045 accuracy\n",
      "DEV AVG EVAL: 1599/2000 = 0.7995 accuracy\n",
      "[learned weights for 77304 features from 2000 examples.]\n"
     ]
    }
   ],
   "source": [
    "y=f1.train(dtr, stepsize=1, numpasses=10, do_averaging=True, devdata=dte)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.3** (4 points)\n",
    "\n",
    "Graph four curves on the same plot, using the **full dataset**:\n",
    "- accuracy of the vanilla perceptron on the training set\n",
    "- accuracy of the vanilla perceptron on the test set\n",
    "- accuracy of the averaged perceptron on the test set\n",
    "- accuracy of your Naive Bayes classifier from HW1 (you don't need to re-run it; just take the best accuracy from your previous results).\n",
    "\n",
    "The x-axis of the plot should show the number of iterations through the\n",
    "training set and the y-axis should show the accuracy of the\n",
    "classifier. For this part of the HW run your code on **the\n",
    "  entire dataset** (all instances). Since Naive Bayes doesn't require\n",
    "multiple passes through the data just produce a single horizontal line\n",
    "showing its overall accuracy. Make sure your plot has a title, a\n",
    "label on the x-axis, a label on the y-axis and a legend showing which\n",
    "line is which. Explain verbally what's happening in this plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out=f1.train(ftr, stepsize=1, numpasses=10, do_averaging=True, devdata=fte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIhWkkJERLSSFCIiopWkEBERrf8D118pcHGRrT4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x45f50240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from numpy import *\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "nb=[]\n",
    "aa=[]\n",
    "nba=.83468\n",
    "for i in range(0,10):\n",
    "         nb.append(nba)\n",
    "         aa.append(i)   \n",
    "tra=out['train_acc']\n",
    "tea=out['test_acc']\n",
    "teavg=out['avg_test_acc']\n",
    "\n",
    "plt.plot(aa,tea,label=\"Test Vanilla \")\n",
    "plt.plot(aa,tra,label=\"Train Vanilla  \")\n",
    "plt.plot(aa,teavg,label=\"Test Averaged \")\n",
    "plt.plot(aa,nb,label=\"Naive Bayes\")\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.ylabel(\"Iterations\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Structured Perceptron with Viterbi (40 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you will implement a part-of-speech tagger for Twitter,\n",
    "using the structured perceptron algorithm.\n",
    "Your system will be not too far off from state of the art performance,\n",
    "coding it all up yourself from scratch!\n",
    "\n",
    "The dataset comes from http://www.ark.cs.cmu.edu/TweetNLP/\n",
    "and is described in the papers listed there (Gimpel et al.~2011 and Owoputi et al.~2013).\n",
    "The Gimpel article describes the tagset; the annotation guidelines on that webpage describe it futher.\n",
    "\n",
    "Your structured perceptron will use your Viterbi implementation from 2.2 as a subroutine.\n",
    "If that's buggy, this will cause many problems here---your perceptron will have really weird behavior.\n",
    "(This happened to us when designing your assignment!)\n",
    "If you have problems, try using the greedy decoding algorithm, which we provide in the starter code.\n",
    "Make sure to note which decoding algorithm you're using in your writeup.\n",
    "\n",
    "The starter code is `structperc.py` and it assumes the two data files `oct27.train`\n",
    "and `oct27.dev` are in the same directory. (For simplicity we're just going to use this `dev` set as our test set.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.1** (2 points)\n",
    "\n",
    "First let's do a little data analysis to establish the **most common tag** baseline accuracy.\n",
    "Using a small script, load up the dev dataset (oct27.dev) using the function `structperc.read_tagging_file` (from `import structperc`). Calculate the following: What is the most common tag, and what would your accuracy be if you predicted it for all tags?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common tag is : \"V\"\n",
      "4823.0\n",
      "Accuracy if we predict the most frequent tag for everything : 0.155712212316\n"
     ]
    }
   ],
   "source": [
    "from structperc import *\n",
    "import structperc as ff\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "ret = read_tagging_file('oct27.dev')\n",
    "tagdict = defaultdict(float)\n",
    "for x,y in ret:\n",
    "    for tag_val in y:\n",
    "        tagdict[tag_val] = tagdict[tag_val] + 1\n",
    "maxTag = max(tagdict.iterkeys(), key=lambda k: tagdict[k])\n",
    "print \"Most common tag is : \\\"\" + maxTag + \"\\\"\"\n",
    "sumVal = 0\n",
    "for key in tagdict:\n",
    "    sumVal += tagdict[key]\n",
    "print sumVal\n",
    "print \"Accuracy if we predict the most frequent tag for everything : \"+str(float(tagdict[maxTag])/sumVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structured perceptron algorithm works very similarly as the classification version you did in the previous question, except the prediction function uses Viterbi as a subroutine, which has to call feature extraction functions for local emissions and transition factors. There also has to be a large overall feature extraction function for an entire structure at once. The following parts will build up these pieces. First, we will focus on inference, not learning.\n",
    "\n",
    "**Question 5.2** (2 points)\n",
    "\n",
    "We provide a barebones version of `local_emission_features`, which calculates the local features for a particular tag at a token position. You can run this function all by itself. Make up an example sentence, and call this function with it, giving it a particular index and candidate tag. Show the code for the function call you made and the function's return value, and explain what the features mean (just a sentence or two)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tag=N_biasterm': 1, 'tag=N_curword=than': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Show the code for function call with output\n",
    "ss=[\"More\",\"than\",\"a\",\"feeling\"]\n",
    "local_emission_features(1, 'N', ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.3** (2 points)\n",
    "\n",
    "Implement `features_for_seq()`, which extracts the full feature vector $f(x,y)$, where $x$is a sentence and $y$ is an entire tagging sequence for that sentence. This will add up the feature vectors from each local emissions features for every position, as well as transition features for every position (there are $N-1$ of them, of course).\n",
    "Show the output on a very short example sentence and example proposed tagging, that's only 2 or 3 words long.\n",
    "\n",
    "To define $f(x,y)$ a little more precisely: If $f^{(B)}(t,x,y)$ means the local emissions feature vector at position $t$ (i.e. the `local_emission_features` function), and $f^{(A)}(y_{t-1},y_{t}, y)$ is the transition feature function for positions $(t-1,t)$ (which just returns a feature vector where everything is zero, except a single element is 1), then the full sequence feature vector will be the vector-sum of all those feature vectors:\n",
    "$$ f(x,y) = \\sum_t^T f^{(B)}(t,x,y) + \\sum_{t=2}^T f^{(A)}(y_{t-1},y_t) $$\n",
    "You implemented $f^{(B)}$ above.  You probably don't need to bother implementing $f^{(A)}$ as a standalone function.  You will have to decide on a particular convention to encode the name of a transition feature.  For example, one way to do it is with string concatenation like this, `\"trans_%s_%s\" % (prevtag, curtag)`, where prevtag and curtag are strings.  Or you could use a python tuple of strings, which works since\n",
    "tuples have the ability to be keys in a python dictionary.\n",
    "\n",
    "In other words: the emissions and transition features will all be in the same vector, just as keys in the dictionary that represents the feature vector.  The transition features are going to be the count of how many times a particular transition (tag bigram) happened.  The emissions features are going to be the vector-sum of all the local emission features, as calculated from `local_emission_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'tag=a _biasterm': 1,\n",
       "             'tag=a _curword=a': 1,\n",
       "             'tag=v_biasterm': 3,\n",
       "             'tag=v_curword=feeling': 1,\n",
       "             'tag=v_curword=more': 1,\n",
       "             'tag=v_curword=than': 1,\n",
       "             'trans_a _v': 1,\n",
       "             'trans_v_a ': 1,\n",
       "             'trans_v_v': 1})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Show the call to your function and output\n",
    "z=features_for_seq([\"more\",\"than\",\"a\",\"feeling\"],[\"v\" ,\"v\" ,\"a \",\"v\"])\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.4** (4 points)\n",
    "\n",
    "Look at the starter code for `calc_factor_scores`, which calculates the A and B score functions that are going to be passed in to your Viterbi implementation from problem 2, in order to do a prediction. The only function it will need to call is `local_emission_features`. It should NOT call `features_for_seq`.  Why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({('!', '!'): 0.0,\n",
       "  ('!', '#'): 0.0,\n",
       "  ('!', '$'): 0.0,\n",
       "  ('!', '&'): 0.0,\n",
       "  ('!', ','): 0.0,\n",
       "  ('!', '@'): 0.0,\n",
       "  ('!', 'A'): 0.0,\n",
       "  ('!', 'D'): 0.0,\n",
       "  ('!', 'E'): 0.0,\n",
       "  ('!', 'G'): 0.0,\n",
       "  ('!', 'L'): 0.0,\n",
       "  ('!', 'M'): 0.0,\n",
       "  ('!', 'N'): 0.0,\n",
       "  ('!', 'O'): 0.0,\n",
       "  ('!', 'P'): 0.0,\n",
       "  ('!', 'R'): 0.0,\n",
       "  ('!', 'S'): 0.0,\n",
       "  ('!', 'T'): 0.0,\n",
       "  ('!', 'U'): 0.0,\n",
       "  ('!', 'V'): 0.0,\n",
       "  ('!', 'X'): 0.0,\n",
       "  ('!', 'Y'): 0.0,\n",
       "  ('!', 'Z'): 0.0,\n",
       "  ('!', '^'): 0.0,\n",
       "  ('#', '!'): 0.0,\n",
       "  ('#', '#'): 0.0,\n",
       "  ('#', '$'): 0.0,\n",
       "  ('#', '&'): 0.0,\n",
       "  ('#', ','): 0.0,\n",
       "  ('#', '@'): 0.0,\n",
       "  ('#', 'A'): 0.0,\n",
       "  ('#', 'D'): 0.0,\n",
       "  ('#', 'E'): 0.0,\n",
       "  ('#', 'G'): 0.0,\n",
       "  ('#', 'L'): 0.0,\n",
       "  ('#', 'M'): 0.0,\n",
       "  ('#', 'N'): 0.0,\n",
       "  ('#', 'O'): 0.0,\n",
       "  ('#', 'P'): 0.0,\n",
       "  ('#', 'R'): 0.0,\n",
       "  ('#', 'S'): 0.0,\n",
       "  ('#', 'T'): 0.0,\n",
       "  ('#', 'U'): 0.0,\n",
       "  ('#', 'V'): 0.0,\n",
       "  ('#', 'X'): 0.0,\n",
       "  ('#', 'Y'): 0.0,\n",
       "  ('#', 'Z'): 0.0,\n",
       "  ('#', '^'): 0.0,\n",
       "  ('$', '!'): 0.0,\n",
       "  ('$', '#'): 0.0,\n",
       "  ('$', '$'): 0.0,\n",
       "  ('$', '&'): 0.0,\n",
       "  ('$', ','): 0.0,\n",
       "  ('$', '@'): 0.0,\n",
       "  ('$', 'A'): 0.0,\n",
       "  ('$', 'D'): 0.0,\n",
       "  ('$', 'E'): 0.0,\n",
       "  ('$', 'G'): 0.0,\n",
       "  ('$', 'L'): 0.0,\n",
       "  ('$', 'M'): 0.0,\n",
       "  ('$', 'N'): 0.0,\n",
       "  ('$', 'O'): 0.0,\n",
       "  ('$', 'P'): 0.0,\n",
       "  ('$', 'R'): 0.0,\n",
       "  ('$', 'S'): 0.0,\n",
       "  ('$', 'T'): 0.0,\n",
       "  ('$', 'U'): 0.0,\n",
       "  ('$', 'V'): 0.0,\n",
       "  ('$', 'X'): 0.0,\n",
       "  ('$', 'Y'): 0.0,\n",
       "  ('$', 'Z'): 0.0,\n",
       "  ('$', '^'): 0.0,\n",
       "  ('&', '!'): 0.0,\n",
       "  ('&', '#'): 0.0,\n",
       "  ('&', '$'): 0.0,\n",
       "  ('&', '&'): 0.0,\n",
       "  ('&', ','): 0.0,\n",
       "  ('&', '@'): 0.0,\n",
       "  ('&', 'A'): 0.0,\n",
       "  ('&', 'D'): 0.0,\n",
       "  ('&', 'E'): 0.0,\n",
       "  ('&', 'G'): 0.0,\n",
       "  ('&', 'L'): 0.0,\n",
       "  ('&', 'M'): 0.0,\n",
       "  ('&', 'N'): 0.0,\n",
       "  ('&', 'O'): 0.0,\n",
       "  ('&', 'P'): 0.0,\n",
       "  ('&', 'R'): 0.0,\n",
       "  ('&', 'S'): 0.0,\n",
       "  ('&', 'T'): 0.0,\n",
       "  ('&', 'U'): 0.0,\n",
       "  ('&', 'V'): 0.0,\n",
       "  ('&', 'X'): 0.0,\n",
       "  ('&', 'Y'): 0.0,\n",
       "  ('&', 'Z'): 0.0,\n",
       "  ('&', '^'): 0.0,\n",
       "  (',', '!'): 0.0,\n",
       "  (',', '#'): 0.0,\n",
       "  (',', '$'): 0.0,\n",
       "  (',', '&'): 0.0,\n",
       "  (',', ','): 0.0,\n",
       "  (',', '@'): 0.0,\n",
       "  (',', 'A'): 0.0,\n",
       "  (',', 'D'): 0.0,\n",
       "  (',', 'E'): 0.0,\n",
       "  (',', 'G'): 0.0,\n",
       "  (',', 'L'): 0.0,\n",
       "  (',', 'M'): 0.0,\n",
       "  (',', 'N'): 0.0,\n",
       "  (',', 'O'): 0.0,\n",
       "  (',', 'P'): 0.0,\n",
       "  (',', 'R'): 0.0,\n",
       "  (',', 'S'): 0.0,\n",
       "  (',', 'T'): 0.0,\n",
       "  (',', 'U'): 0.0,\n",
       "  (',', 'V'): 0.0,\n",
       "  (',', 'X'): 0.0,\n",
       "  (',', 'Y'): 0.0,\n",
       "  (',', 'Z'): 0.0,\n",
       "  (',', '^'): 0.0,\n",
       "  ('@', '!'): 0.0,\n",
       "  ('@', '#'): 0.0,\n",
       "  ('@', '$'): 0.0,\n",
       "  ('@', '&'): 0.0,\n",
       "  ('@', ','): 0.0,\n",
       "  ('@', '@'): 0.0,\n",
       "  ('@', 'A'): 0.0,\n",
       "  ('@', 'D'): 0.0,\n",
       "  ('@', 'E'): 0.0,\n",
       "  ('@', 'G'): 0.0,\n",
       "  ('@', 'L'): 0.0,\n",
       "  ('@', 'M'): 0.0,\n",
       "  ('@', 'N'): 0.0,\n",
       "  ('@', 'O'): 0.0,\n",
       "  ('@', 'P'): 0.0,\n",
       "  ('@', 'R'): 0.0,\n",
       "  ('@', 'S'): 0.0,\n",
       "  ('@', 'T'): 0.0,\n",
       "  ('@', 'U'): 0.0,\n",
       "  ('@', 'V'): 0.0,\n",
       "  ('@', 'X'): 0.0,\n",
       "  ('@', 'Y'): 0.0,\n",
       "  ('@', 'Z'): 0.0,\n",
       "  ('@', '^'): 0.0,\n",
       "  ('A', '!'): 0.0,\n",
       "  ('A', '#'): 0.0,\n",
       "  ('A', '$'): 0.0,\n",
       "  ('A', '&'): 0.0,\n",
       "  ('A', ','): 0.0,\n",
       "  ('A', '@'): 0.0,\n",
       "  ('A', 'A'): 0.0,\n",
       "  ('A', 'D'): 0.0,\n",
       "  ('A', 'E'): 0.0,\n",
       "  ('A', 'G'): 0.0,\n",
       "  ('A', 'L'): 0.0,\n",
       "  ('A', 'M'): 0.0,\n",
       "  ('A', 'N'): 0.0,\n",
       "  ('A', 'O'): 0.0,\n",
       "  ('A', 'P'): 0.0,\n",
       "  ('A', 'R'): 0.0,\n",
       "  ('A', 'S'): 0.0,\n",
       "  ('A', 'T'): 0.0,\n",
       "  ('A', 'U'): 0.0,\n",
       "  ('A', 'V'): 0.0,\n",
       "  ('A', 'X'): 0.0,\n",
       "  ('A', 'Y'): 0.0,\n",
       "  ('A', 'Z'): 0.0,\n",
       "  ('A', '^'): 0.0,\n",
       "  ('D', '!'): 0.0,\n",
       "  ('D', '#'): 0.0,\n",
       "  ('D', '$'): 0.0,\n",
       "  ('D', '&'): 0.0,\n",
       "  ('D', ','): 0.0,\n",
       "  ('D', '@'): 0.0,\n",
       "  ('D', 'A'): 0.0,\n",
       "  ('D', 'D'): 0.0,\n",
       "  ('D', 'E'): 0.0,\n",
       "  ('D', 'G'): 0.0,\n",
       "  ('D', 'L'): 0.0,\n",
       "  ('D', 'M'): 0.0,\n",
       "  ('D', 'N'): 0.0,\n",
       "  ('D', 'O'): 0.0,\n",
       "  ('D', 'P'): 0.0,\n",
       "  ('D', 'R'): 0.0,\n",
       "  ('D', 'S'): 0.0,\n",
       "  ('D', 'T'): 0.0,\n",
       "  ('D', 'U'): 0.0,\n",
       "  ('D', 'V'): 0.0,\n",
       "  ('D', 'X'): 0.0,\n",
       "  ('D', 'Y'): 0.0,\n",
       "  ('D', 'Z'): 0.0,\n",
       "  ('D', '^'): 0.0,\n",
       "  ('E', '!'): 0.0,\n",
       "  ('E', '#'): 0.0,\n",
       "  ('E', '$'): 0.0,\n",
       "  ('E', '&'): 0.0,\n",
       "  ('E', ','): 0.0,\n",
       "  ('E', '@'): 0.0,\n",
       "  ('E', 'A'): 0.0,\n",
       "  ('E', 'D'): 0.0,\n",
       "  ('E', 'E'): 0.0,\n",
       "  ('E', 'G'): 0.0,\n",
       "  ('E', 'L'): 0.0,\n",
       "  ('E', 'M'): 0.0,\n",
       "  ('E', 'N'): 0.0,\n",
       "  ('E', 'O'): 0.0,\n",
       "  ('E', 'P'): 0.0,\n",
       "  ('E', 'R'): 0.0,\n",
       "  ('E', 'S'): 0.0,\n",
       "  ('E', 'T'): 0.0,\n",
       "  ('E', 'U'): 0.0,\n",
       "  ('E', 'V'): 0.0,\n",
       "  ('E', 'X'): 0.0,\n",
       "  ('E', 'Y'): 0.0,\n",
       "  ('E', 'Z'): 0.0,\n",
       "  ('E', '^'): 0.0,\n",
       "  ('G', '!'): 0.0,\n",
       "  ('G', '#'): 0.0,\n",
       "  ('G', '$'): 0.0,\n",
       "  ('G', '&'): 0.0,\n",
       "  ('G', ','): 0.0,\n",
       "  ('G', '@'): 0.0,\n",
       "  ('G', 'A'): 0.0,\n",
       "  ('G', 'D'): 0.0,\n",
       "  ('G', 'E'): 0.0,\n",
       "  ('G', 'G'): 0.0,\n",
       "  ('G', 'L'): 0.0,\n",
       "  ('G', 'M'): 0.0,\n",
       "  ('G', 'N'): 0.0,\n",
       "  ('G', 'O'): 0.0,\n",
       "  ('G', 'P'): 0.0,\n",
       "  ('G', 'R'): 0.0,\n",
       "  ('G', 'S'): 0.0,\n",
       "  ('G', 'T'): 0.0,\n",
       "  ('G', 'U'): 0.0,\n",
       "  ('G', 'V'): 0.0,\n",
       "  ('G', 'X'): 0.0,\n",
       "  ('G', 'Y'): 0.0,\n",
       "  ('G', 'Z'): 0.0,\n",
       "  ('G', '^'): 0.0,\n",
       "  ('L', '!'): 0.0,\n",
       "  ('L', '#'): 0.0,\n",
       "  ('L', '$'): 0.0,\n",
       "  ('L', '&'): 0.0,\n",
       "  ('L', ','): 0.0,\n",
       "  ('L', '@'): 0.0,\n",
       "  ('L', 'A'): 0.0,\n",
       "  ('L', 'D'): 0.0,\n",
       "  ('L', 'E'): 0.0,\n",
       "  ('L', 'G'): 0.0,\n",
       "  ('L', 'L'): 0.0,\n",
       "  ('L', 'M'): 0.0,\n",
       "  ('L', 'N'): 0.0,\n",
       "  ('L', 'O'): 0.0,\n",
       "  ('L', 'P'): 0.0,\n",
       "  ('L', 'R'): 0.0,\n",
       "  ('L', 'S'): 0.0,\n",
       "  ('L', 'T'): 0.0,\n",
       "  ('L', 'U'): 0.0,\n",
       "  ('L', 'V'): 0.0,\n",
       "  ('L', 'X'): 0.0,\n",
       "  ('L', 'Y'): 0.0,\n",
       "  ('L', 'Z'): 0.0,\n",
       "  ('L', '^'): 0.0,\n",
       "  ('M', '!'): 0.0,\n",
       "  ('M', '#'): 0.0,\n",
       "  ('M', '$'): 0.0,\n",
       "  ('M', '&'): 0.0,\n",
       "  ('M', ','): 0.0,\n",
       "  ('M', '@'): 0.0,\n",
       "  ('M', 'A'): 0.0,\n",
       "  ('M', 'D'): 0.0,\n",
       "  ('M', 'E'): 0.0,\n",
       "  ('M', 'G'): 0.0,\n",
       "  ('M', 'L'): 0.0,\n",
       "  ('M', 'M'): 0.0,\n",
       "  ('M', 'N'): 0.0,\n",
       "  ('M', 'O'): 0.0,\n",
       "  ('M', 'P'): 0.0,\n",
       "  ('M', 'R'): 0.0,\n",
       "  ('M', 'S'): 0.0,\n",
       "  ('M', 'T'): 0.0,\n",
       "  ('M', 'U'): 0.0,\n",
       "  ('M', 'V'): 0.0,\n",
       "  ('M', 'X'): 0.0,\n",
       "  ('M', 'Y'): 0.0,\n",
       "  ('M', 'Z'): 0.0,\n",
       "  ('M', '^'): 0.0,\n",
       "  ('N', '!'): 0.0,\n",
       "  ('N', '#'): 0.0,\n",
       "  ('N', '$'): 0.0,\n",
       "  ('N', '&'): 0.0,\n",
       "  ('N', ','): 0.0,\n",
       "  ('N', '@'): 0.0,\n",
       "  ('N', 'A'): 0.0,\n",
       "  ('N', 'D'): 0.0,\n",
       "  ('N', 'E'): 0.0,\n",
       "  ('N', 'G'): 0.0,\n",
       "  ('N', 'L'): 0.0,\n",
       "  ('N', 'M'): 0.0,\n",
       "  ('N', 'N'): 0.0,\n",
       "  ('N', 'O'): 0.0,\n",
       "  ('N', 'P'): 0.0,\n",
       "  ('N', 'R'): 0.0,\n",
       "  ('N', 'S'): 0.0,\n",
       "  ('N', 'T'): 0.0,\n",
       "  ('N', 'U'): 0.0,\n",
       "  ('N', 'V'): 0.0,\n",
       "  ('N', 'X'): 0.0,\n",
       "  ('N', 'Y'): 0.0,\n",
       "  ('N', 'Z'): 0.0,\n",
       "  ('N', '^'): 0.0,\n",
       "  ('O', '!'): 0.0,\n",
       "  ('O', '#'): 0.0,\n",
       "  ('O', '$'): 0.0,\n",
       "  ('O', '&'): 0.0,\n",
       "  ('O', ','): 0.0,\n",
       "  ('O', '@'): 0.0,\n",
       "  ('O', 'A'): 0.0,\n",
       "  ('O', 'D'): 0.0,\n",
       "  ('O', 'E'): 0.0,\n",
       "  ('O', 'G'): 0.0,\n",
       "  ('O', 'L'): 0.0,\n",
       "  ('O', 'M'): 0.0,\n",
       "  ('O', 'N'): 0.0,\n",
       "  ('O', 'O'): 0.0,\n",
       "  ('O', 'P'): 0.0,\n",
       "  ('O', 'R'): 0.0,\n",
       "  ('O', 'S'): 0.0,\n",
       "  ('O', 'T'): 0.0,\n",
       "  ('O', 'U'): 0.0,\n",
       "  ('O', 'V'): 0.0,\n",
       "  ('O', 'X'): 0.0,\n",
       "  ('O', 'Y'): 0.0,\n",
       "  ('O', 'Z'): 0.0,\n",
       "  ('O', '^'): 0.0,\n",
       "  ('P', '!'): 0.0,\n",
       "  ('P', '#'): 0.0,\n",
       "  ('P', '$'): 0.0,\n",
       "  ('P', '&'): 0.0,\n",
       "  ('P', ','): 0.0,\n",
       "  ('P', '@'): 0.0,\n",
       "  ('P', 'A'): 0.0,\n",
       "  ('P', 'D'): 0.0,\n",
       "  ('P', 'E'): 0.0,\n",
       "  ('P', 'G'): 0.0,\n",
       "  ('P', 'L'): 0.0,\n",
       "  ('P', 'M'): 0.0,\n",
       "  ('P', 'N'): 0.0,\n",
       "  ('P', 'O'): 0.0,\n",
       "  ('P', 'P'): 0.0,\n",
       "  ('P', 'R'): 0.0,\n",
       "  ('P', 'S'): 0.0,\n",
       "  ('P', 'T'): 0.0,\n",
       "  ('P', 'U'): 0.0,\n",
       "  ('P', 'V'): 0.0,\n",
       "  ('P', 'X'): 0.0,\n",
       "  ('P', 'Y'): 0.0,\n",
       "  ('P', 'Z'): 0.0,\n",
       "  ('P', '^'): 0.0,\n",
       "  ('R', '!'): 0.0,\n",
       "  ('R', '#'): 0.0,\n",
       "  ('R', '$'): 0.0,\n",
       "  ('R', '&'): 0.0,\n",
       "  ('R', ','): 0.0,\n",
       "  ('R', '@'): 0.0,\n",
       "  ('R', 'A'): 0.0,\n",
       "  ('R', 'D'): 0.0,\n",
       "  ('R', 'E'): 0.0,\n",
       "  ('R', 'G'): 0.0,\n",
       "  ('R', 'L'): 0.0,\n",
       "  ('R', 'M'): 0.0,\n",
       "  ('R', 'N'): 0.0,\n",
       "  ('R', 'O'): 0.0,\n",
       "  ('R', 'P'): 0.0,\n",
       "  ('R', 'R'): 0.0,\n",
       "  ('R', 'S'): 0.0,\n",
       "  ('R', 'T'): 0.0,\n",
       "  ('R', 'U'): 0.0,\n",
       "  ('R', 'V'): 0.0,\n",
       "  ('R', 'X'): 0.0,\n",
       "  ('R', 'Y'): 0.0,\n",
       "  ('R', 'Z'): 0.0,\n",
       "  ('R', '^'): 0.0,\n",
       "  ('S', '!'): 0.0,\n",
       "  ('S', '#'): 0.0,\n",
       "  ('S', '$'): 0.0,\n",
       "  ('S', '&'): 0.0,\n",
       "  ('S', ','): 0.0,\n",
       "  ('S', '@'): 0.0,\n",
       "  ('S', 'A'): 0.0,\n",
       "  ('S', 'D'): 0.0,\n",
       "  ('S', 'E'): 0.0,\n",
       "  ('S', 'G'): 0.0,\n",
       "  ('S', 'L'): 0.0,\n",
       "  ('S', 'M'): 0.0,\n",
       "  ('S', 'N'): 0.0,\n",
       "  ('S', 'O'): 0.0,\n",
       "  ('S', 'P'): 0.0,\n",
       "  ('S', 'R'): 0.0,\n",
       "  ('S', 'S'): 0.0,\n",
       "  ('S', 'T'): 0.0,\n",
       "  ('S', 'U'): 0.0,\n",
       "  ('S', 'V'): 0.0,\n",
       "  ('S', 'X'): 0.0,\n",
       "  ('S', 'Y'): 0.0,\n",
       "  ('S', 'Z'): 0.0,\n",
       "  ('S', '^'): 0.0,\n",
       "  ('T', '!'): 0.0,\n",
       "  ('T', '#'): 0.0,\n",
       "  ('T', '$'): 0.0,\n",
       "  ('T', '&'): 0.0,\n",
       "  ('T', ','): 0.0,\n",
       "  ('T', '@'): 0.0,\n",
       "  ('T', 'A'): 0.0,\n",
       "  ('T', 'D'): 0.0,\n",
       "  ('T', 'E'): 0.0,\n",
       "  ('T', 'G'): 0.0,\n",
       "  ('T', 'L'): 0.0,\n",
       "  ('T', 'M'): 0.0,\n",
       "  ('T', 'N'): 0.0,\n",
       "  ('T', 'O'): 0.0,\n",
       "  ('T', 'P'): 0.0,\n",
       "  ('T', 'R'): 0.0,\n",
       "  ('T', 'S'): 0.0,\n",
       "  ('T', 'T'): 0.0,\n",
       "  ('T', 'U'): 0.0,\n",
       "  ('T', 'V'): 0.0,\n",
       "  ('T', 'X'): 0.0,\n",
       "  ('T', 'Y'): 0.0,\n",
       "  ('T', 'Z'): 0.0,\n",
       "  ('T', '^'): 0.0,\n",
       "  ('U', '!'): 0.0,\n",
       "  ('U', '#'): 0.0,\n",
       "  ('U', '$'): 0.0,\n",
       "  ('U', '&'): 0.0,\n",
       "  ('U', ','): 0.0,\n",
       "  ('U', '@'): 0.0,\n",
       "  ('U', 'A'): 0.0,\n",
       "  ('U', 'D'): 0.0,\n",
       "  ('U', 'E'): 0.0,\n",
       "  ('U', 'G'): 0.0,\n",
       "  ('U', 'L'): 0.0,\n",
       "  ('U', 'M'): 0.0,\n",
       "  ('U', 'N'): 0.0,\n",
       "  ('U', 'O'): 0.0,\n",
       "  ('U', 'P'): 0.0,\n",
       "  ('U', 'R'): 0.0,\n",
       "  ('U', 'S'): 0.0,\n",
       "  ('U', 'T'): 0.0,\n",
       "  ('U', 'U'): 0.0,\n",
       "  ('U', 'V'): 0.0,\n",
       "  ('U', 'X'): 0.0,\n",
       "  ('U', 'Y'): 0.0,\n",
       "  ('U', 'Z'): 0.0,\n",
       "  ('U', '^'): 0.0,\n",
       "  ('V', '!'): 0.0,\n",
       "  ('V', '#'): 0.0,\n",
       "  ('V', '$'): 0.0,\n",
       "  ('V', '&'): 0.0,\n",
       "  ('V', ','): 0.0,\n",
       "  ('V', '@'): 0.0,\n",
       "  ('V', 'A'): 0.0,\n",
       "  ('V', 'D'): 0.0,\n",
       "  ('V', 'E'): 0.0,\n",
       "  ('V', 'G'): 0.0,\n",
       "  ('V', 'L'): 0.0,\n",
       "  ('V', 'M'): 0.0,\n",
       "  ('V', 'N'): 0.0,\n",
       "  ('V', 'O'): 0.0,\n",
       "  ('V', 'P'): 0.0,\n",
       "  ('V', 'R'): 0.0,\n",
       "  ('V', 'S'): 0.0,\n",
       "  ('V', 'T'): 0.0,\n",
       "  ('V', 'U'): 0.0,\n",
       "  ('V', 'V'): 0.0,\n",
       "  ('V', 'X'): 0.0,\n",
       "  ('V', 'Y'): 0.0,\n",
       "  ('V', 'Z'): 0.0,\n",
       "  ('V', '^'): 0.0,\n",
       "  ('X', '!'): 0.0,\n",
       "  ('X', '#'): 0.0,\n",
       "  ('X', '$'): 0.0,\n",
       "  ('X', '&'): 0.0,\n",
       "  ('X', ','): 0.0,\n",
       "  ('X', '@'): 0.0,\n",
       "  ('X', 'A'): 0.0,\n",
       "  ('X', 'D'): 0.0,\n",
       "  ('X', 'E'): 0.0,\n",
       "  ('X', 'G'): 0.0,\n",
       "  ('X', 'L'): 0.0,\n",
       "  ('X', 'M'): 0.0,\n",
       "  ('X', 'N'): 0.0,\n",
       "  ('X', 'O'): 0.0,\n",
       "  ('X', 'P'): 0.0,\n",
       "  ('X', 'R'): 0.0,\n",
       "  ('X', 'S'): 0.0,\n",
       "  ('X', 'T'): 0.0,\n",
       "  ('X', 'U'): 0.0,\n",
       "  ('X', 'V'): 0.0,\n",
       "  ('X', 'X'): 0.0,\n",
       "  ('X', 'Y'): 0.0,\n",
       "  ('X', 'Z'): 0.0,\n",
       "  ('X', '^'): 0.0,\n",
       "  ('Y', '!'): 0.0,\n",
       "  ('Y', '#'): 0.0,\n",
       "  ('Y', '$'): 0.0,\n",
       "  ('Y', '&'): 0.0,\n",
       "  ('Y', ','): 0.0,\n",
       "  ('Y', '@'): 0.0,\n",
       "  ('Y', 'A'): 0.0,\n",
       "  ('Y', 'D'): 0.0,\n",
       "  ('Y', 'E'): 0.0,\n",
       "  ('Y', 'G'): 0.0,\n",
       "  ('Y', 'L'): 0.0,\n",
       "  ('Y', 'M'): 0.0,\n",
       "  ('Y', 'N'): 0.0,\n",
       "  ('Y', 'O'): 0.0,\n",
       "  ('Y', 'P'): 0.0,\n",
       "  ('Y', 'R'): 0.0,\n",
       "  ('Y', 'S'): 0.0,\n",
       "  ('Y', 'T'): 0.0,\n",
       "  ('Y', 'U'): 0.0,\n",
       "  ('Y', 'V'): 0.0,\n",
       "  ('Y', 'X'): 0.0,\n",
       "  ('Y', 'Y'): 0.0,\n",
       "  ('Y', 'Z'): 0.0,\n",
       "  ('Y', '^'): 0.0,\n",
       "  ('Z', '!'): 0.0,\n",
       "  ('Z', '#'): 0.0,\n",
       "  ('Z', '$'): 0.0,\n",
       "  ('Z', '&'): 0.0,\n",
       "  ('Z', ','): 0.0,\n",
       "  ('Z', '@'): 0.0,\n",
       "  ('Z', 'A'): 0.0,\n",
       "  ('Z', 'D'): 0.0,\n",
       "  ('Z', 'E'): 0.0,\n",
       "  ('Z', 'G'): 0.0,\n",
       "  ('Z', 'L'): 0.0,\n",
       "  ('Z', 'M'): 0.0,\n",
       "  ('Z', 'N'): 0.0,\n",
       "  ('Z', 'O'): 0.0,\n",
       "  ('Z', 'P'): 0.0,\n",
       "  ('Z', 'R'): 0.0,\n",
       "  ('Z', 'S'): 0.0,\n",
       "  ('Z', 'T'): 0.0,\n",
       "  ('Z', 'U'): 0.0,\n",
       "  ('Z', 'V'): 0.0,\n",
       "  ('Z', 'X'): 0.0,\n",
       "  ('Z', 'Y'): 0.0,\n",
       "  ('Z', 'Z'): 0.0,\n",
       "  ('Z', '^'): 0.0,\n",
       "  ('^', '!'): 0.0,\n",
       "  ('^', '#'): 0.0,\n",
       "  ('^', '$'): 0.0,\n",
       "  ('^', '&'): 0.0,\n",
       "  ('^', ','): 0.0,\n",
       "  ('^', '@'): 0.0,\n",
       "  ('^', 'A'): 0.0,\n",
       "  ('^', 'D'): 0.0,\n",
       "  ('^', 'E'): 0.0,\n",
       "  ('^', 'G'): 0.0,\n",
       "  ('^', 'L'): 0.0,\n",
       "  ('^', 'M'): 0.0,\n",
       "  ('^', 'N'): 0.0,\n",
       "  ('^', 'O'): 0.0,\n",
       "  ('^', 'P'): 0.0,\n",
       "  ('^', 'R'): 0.0,\n",
       "  ('^', 'S'): 0.0,\n",
       "  ('^', 'T'): 0.0,\n",
       "  ('^', 'U'): 0.0,\n",
       "  ('^', 'V'): 0.0,\n",
       "  ('^', 'X'): 0.0,\n",
       "  ('^', 'Y'): 0.0,\n",
       "  ('^', 'Z'): 0.0,\n",
       "  ('^', '^'): 0.0},\n",
       " [defaultdict(float,\n",
       "              {'!': 0,\n",
       "               '#': 0,\n",
       "               '$': 0,\n",
       "               '&': 0,\n",
       "               ',': 0,\n",
       "               '@': 0,\n",
       "               'A': 0,\n",
       "               'D': 0,\n",
       "               'E': 0,\n",
       "               'G': 0,\n",
       "               'L': 0,\n",
       "               'M': 0,\n",
       "               'N': 0,\n",
       "               'O': 0,\n",
       "               'P': 0,\n",
       "               'R': 0,\n",
       "               'S': 0,\n",
       "               'T': 0,\n",
       "               'U': 0,\n",
       "               'V': 0,\n",
       "               'X': 0,\n",
       "               'Y': 0,\n",
       "               'Z': 0,\n",
       "               '^': 0}),\n",
       "  defaultdict(float,\n",
       "              {'!': 0,\n",
       "               '#': 0,\n",
       "               '$': 0,\n",
       "               '&': 0,\n",
       "               ',': 0,\n",
       "               '@': 0,\n",
       "               'A': 0,\n",
       "               'D': 0,\n",
       "               'E': 0,\n",
       "               'G': 0,\n",
       "               'L': 0,\n",
       "               'M': 0,\n",
       "               'N': 0,\n",
       "               'O': 0,\n",
       "               'P': 0,\n",
       "               'R': 0,\n",
       "               'S': 0,\n",
       "               'T': 0,\n",
       "               'U': 0,\n",
       "               'V': 0,\n",
       "               'X': 0,\n",
       "               'Y': 0,\n",
       "               'Z': 0,\n",
       "               '^': 0}),\n",
       "  defaultdict(float,\n",
       "              {'!': 0,\n",
       "               '#': 0,\n",
       "               '$': 0,\n",
       "               '&': 0,\n",
       "               ',': 0,\n",
       "               '@': 0,\n",
       "               'A': 0,\n",
       "               'D': 0,\n",
       "               'E': 0,\n",
       "               'G': 0,\n",
       "               'L': 0,\n",
       "               'M': 0,\n",
       "               'N': 0,\n",
       "               'O': 0,\n",
       "               'P': 0,\n",
       "               'R': 0,\n",
       "               'S': 0,\n",
       "               'T': 0,\n",
       "               'U': 0,\n",
       "               'V': 0,\n",
       "               'X': 0,\n",
       "               'Y': 0,\n",
       "               'Z': 0,\n",
       "               '^': 0}),\n",
       "  defaultdict(float,\n",
       "              {'!': 0,\n",
       "               '#': 0,\n",
       "               '$': 0,\n",
       "               '&': 0,\n",
       "               ',': 0,\n",
       "               '@': 0,\n",
       "               'A': 0,\n",
       "               'D': 0,\n",
       "               'E': 0,\n",
       "               'G': 0,\n",
       "               'L': 0,\n",
       "               'M': 0,\n",
       "               'N': 0,\n",
       "               'O': 0,\n",
       "               'P': 0,\n",
       "               'R': 0,\n",
       "               'S': 0,\n",
       "               'T': 0,\n",
       "               'U': 0,\n",
       "               'V': 0,\n",
       "               'X': 0,\n",
       "               'Y': 0,\n",
       "               'Z': 0,\n",
       "               '^': 0})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w=defaultdict(float,more=2,than=1,a=6,feeling=10)\n",
    "calc_factor_scores(ss,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.5** (6 points)\n",
    "\n",
    "Implement `calc_factor_scores`. Make up a simple example (2 or 3 words long), with a simple model with at least some \n",
    "nonzero features (you might want to use a `defaultdict(float)`, so you don't have to fill up a dict with dummy values \n",
    "for all possible transitions), and show your call to this function and the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({('!', '!'): 0.0,\n",
       "  ('!', '#'): 0.0,\n",
       "  ('!', '$'): 0.0,\n",
       "  ('!', '&'): 0.0,\n",
       "  ('!', ','): 0.0,\n",
       "  ('!', '@'): 0.0,\n",
       "  ('!', 'A'): 0.0,\n",
       "  ('!', 'D'): 0.0,\n",
       "  ('!', 'E'): 0.0,\n",
       "  ('!', 'G'): 0.0,\n",
       "  ('!', 'L'): 0.0,\n",
       "  ('!', 'M'): 0.0,\n",
       "  ('!', 'N'): 0.0,\n",
       "  ('!', 'O'): 0.0,\n",
       "  ('!', 'P'): 0.0,\n",
       "  ('!', 'R'): 0.0,\n",
       "  ('!', 'S'): 0.0,\n",
       "  ('!', 'T'): 0.0,\n",
       "  ('!', 'U'): 0.0,\n",
       "  ('!', 'V'): 0.0,\n",
       "  ('!', 'X'): 0.0,\n",
       "  ('!', 'Y'): 0.0,\n",
       "  ('!', 'Z'): 0.0,\n",
       "  ('!', '^'): 0.0,\n",
       "  ('#', '!'): 0.0,\n",
       "  ('#', '#'): 0.0,\n",
       "  ('#', '$'): 0.0,\n",
       "  ('#', '&'): 0.0,\n",
       "  ('#', ','): 0.0,\n",
       "  ('#', '@'): 0.0,\n",
       "  ('#', 'A'): 0.0,\n",
       "  ('#', 'D'): 0.0,\n",
       "  ('#', 'E'): 0.0,\n",
       "  ('#', 'G'): 0.0,\n",
       "  ('#', 'L'): 0.0,\n",
       "  ('#', 'M'): 0.0,\n",
       "  ('#', 'N'): 0.0,\n",
       "  ('#', 'O'): 0.0,\n",
       "  ('#', 'P'): 0.0,\n",
       "  ('#', 'R'): 0.0,\n",
       "  ('#', 'S'): 0.0,\n",
       "  ('#', 'T'): 0.0,\n",
       "  ('#', 'U'): 0.0,\n",
       "  ('#', 'V'): 0.0,\n",
       "  ('#', 'X'): 0.0,\n",
       "  ('#', 'Y'): 0.0,\n",
       "  ('#', 'Z'): 0.0,\n",
       "  ('#', '^'): 0.0,\n",
       "  ('$', '!'): 0.0,\n",
       "  ('$', '#'): 0.0,\n",
       "  ('$', '$'): 0.0,\n",
       "  ('$', '&'): 0.0,\n",
       "  ('$', ','): 0.0,\n",
       "  ('$', '@'): 0.0,\n",
       "  ('$', 'A'): 0.0,\n",
       "  ('$', 'D'): 0.0,\n",
       "  ('$', 'E'): 0.0,\n",
       "  ('$', 'G'): 0.0,\n",
       "  ('$', 'L'): 0.0,\n",
       "  ('$', 'M'): 0.0,\n",
       "  ('$', 'N'): 0.0,\n",
       "  ('$', 'O'): 0.0,\n",
       "  ('$', 'P'): 0.0,\n",
       "  ('$', 'R'): 0.0,\n",
       "  ('$', 'S'): 0.0,\n",
       "  ('$', 'T'): 0.0,\n",
       "  ('$', 'U'): 0.0,\n",
       "  ('$', 'V'): 0.0,\n",
       "  ('$', 'X'): 0.0,\n",
       "  ('$', 'Y'): 0.0,\n",
       "  ('$', 'Z'): 0.0,\n",
       "  ('$', '^'): 0.0,\n",
       "  ('&', '!'): 0.0,\n",
       "  ('&', '#'): 0.0,\n",
       "  ('&', '$'): 0.0,\n",
       "  ('&', '&'): 0.0,\n",
       "  ('&', ','): 0.0,\n",
       "  ('&', '@'): 0.0,\n",
       "  ('&', 'A'): 0.0,\n",
       "  ('&', 'D'): 0.0,\n",
       "  ('&', 'E'): 0.0,\n",
       "  ('&', 'G'): 0.0,\n",
       "  ('&', 'L'): 0.0,\n",
       "  ('&', 'M'): 0.0,\n",
       "  ('&', 'N'): 0.0,\n",
       "  ('&', 'O'): 0.0,\n",
       "  ('&', 'P'): 0.0,\n",
       "  ('&', 'R'): 0.0,\n",
       "  ('&', 'S'): 0.0,\n",
       "  ('&', 'T'): 0.0,\n",
       "  ('&', 'U'): 0.0,\n",
       "  ('&', 'V'): 0.0,\n",
       "  ('&', 'X'): 0.0,\n",
       "  ('&', 'Y'): 0.0,\n",
       "  ('&', 'Z'): 0.0,\n",
       "  ('&', '^'): 0.0,\n",
       "  (',', '!'): 0.0,\n",
       "  (',', '#'): 0.0,\n",
       "  (',', '$'): 0.0,\n",
       "  (',', '&'): 0.0,\n",
       "  (',', ','): 0.0,\n",
       "  (',', '@'): 0.0,\n",
       "  (',', 'A'): 0.0,\n",
       "  (',', 'D'): 0.0,\n",
       "  (',', 'E'): 0.0,\n",
       "  (',', 'G'): 0.0,\n",
       "  (',', 'L'): 0.0,\n",
       "  (',', 'M'): 0.0,\n",
       "  (',', 'N'): 0.0,\n",
       "  (',', 'O'): 0.0,\n",
       "  (',', 'P'): 0.0,\n",
       "  (',', 'R'): 0.0,\n",
       "  (',', 'S'): 0.0,\n",
       "  (',', 'T'): 0.0,\n",
       "  (',', 'U'): 0.0,\n",
       "  (',', 'V'): 0.0,\n",
       "  (',', 'X'): 0.0,\n",
       "  (',', 'Y'): 0.0,\n",
       "  (',', 'Z'): 0.0,\n",
       "  (',', '^'): 0.0,\n",
       "  ('@', '!'): 0.0,\n",
       "  ('@', '#'): 0.0,\n",
       "  ('@', '$'): 0.0,\n",
       "  ('@', '&'): 0.0,\n",
       "  ('@', ','): 0.0,\n",
       "  ('@', '@'): 0.0,\n",
       "  ('@', 'A'): 0.0,\n",
       "  ('@', 'D'): 0.0,\n",
       "  ('@', 'E'): 0.0,\n",
       "  ('@', 'G'): 0.0,\n",
       "  ('@', 'L'): 0.0,\n",
       "  ('@', 'M'): 0.0,\n",
       "  ('@', 'N'): 0.0,\n",
       "  ('@', 'O'): 0.0,\n",
       "  ('@', 'P'): 0.0,\n",
       "  ('@', 'R'): 0.0,\n",
       "  ('@', 'S'): 0.0,\n",
       "  ('@', 'T'): 0.0,\n",
       "  ('@', 'U'): 0.0,\n",
       "  ('@', 'V'): 0.0,\n",
       "  ('@', 'X'): 0.0,\n",
       "  ('@', 'Y'): 0.0,\n",
       "  ('@', 'Z'): 0.0,\n",
       "  ('@', '^'): 0.0,\n",
       "  ('A', '!'): 0.0,\n",
       "  ('A', '#'): 0.0,\n",
       "  ('A', '$'): 0.0,\n",
       "  ('A', '&'): 0.0,\n",
       "  ('A', ','): 0.0,\n",
       "  ('A', '@'): 0.0,\n",
       "  ('A', 'A'): 0.0,\n",
       "  ('A', 'D'): 0.0,\n",
       "  ('A', 'E'): 0.0,\n",
       "  ('A', 'G'): 0.0,\n",
       "  ('A', 'L'): 0.0,\n",
       "  ('A', 'M'): 0.0,\n",
       "  ('A', 'N'): 0.0,\n",
       "  ('A', 'O'): 0.0,\n",
       "  ('A', 'P'): 0.0,\n",
       "  ('A', 'R'): 0.0,\n",
       "  ('A', 'S'): 0.0,\n",
       "  ('A', 'T'): 0.0,\n",
       "  ('A', 'U'): 0.0,\n",
       "  ('A', 'V'): 0.0,\n",
       "  ('A', 'X'): 0.0,\n",
       "  ('A', 'Y'): 0.0,\n",
       "  ('A', 'Z'): 0.0,\n",
       "  ('A', '^'): 0.0,\n",
       "  ('D', '!'): 0.0,\n",
       "  ('D', '#'): 0.0,\n",
       "  ('D', '$'): 0.0,\n",
       "  ('D', '&'): 0.0,\n",
       "  ('D', ','): 0.0,\n",
       "  ('D', '@'): 0.0,\n",
       "  ('D', 'A'): 0.0,\n",
       "  ('D', 'D'): 0.0,\n",
       "  ('D', 'E'): 0.0,\n",
       "  ('D', 'G'): 0.0,\n",
       "  ('D', 'L'): 0.0,\n",
       "  ('D', 'M'): 0.0,\n",
       "  ('D', 'N'): 0.0,\n",
       "  ('D', 'O'): 0.0,\n",
       "  ('D', 'P'): 0.0,\n",
       "  ('D', 'R'): 0.0,\n",
       "  ('D', 'S'): 0.0,\n",
       "  ('D', 'T'): 0.0,\n",
       "  ('D', 'U'): 0.0,\n",
       "  ('D', 'V'): 0.0,\n",
       "  ('D', 'X'): 0.0,\n",
       "  ('D', 'Y'): 0.0,\n",
       "  ('D', 'Z'): 0.0,\n",
       "  ('D', '^'): 0.0,\n",
       "  ('E', '!'): 0.0,\n",
       "  ('E', '#'): 0.0,\n",
       "  ('E', '$'): 0.0,\n",
       "  ('E', '&'): 0.0,\n",
       "  ('E', ','): 0.0,\n",
       "  ('E', '@'): 0.0,\n",
       "  ('E', 'A'): 0.0,\n",
       "  ('E', 'D'): 0.0,\n",
       "  ('E', 'E'): 0.0,\n",
       "  ('E', 'G'): 0.0,\n",
       "  ('E', 'L'): 0.0,\n",
       "  ('E', 'M'): 0.0,\n",
       "  ('E', 'N'): 0.0,\n",
       "  ('E', 'O'): 0.0,\n",
       "  ('E', 'P'): 0.0,\n",
       "  ('E', 'R'): 0.0,\n",
       "  ('E', 'S'): 0.0,\n",
       "  ('E', 'T'): 0.0,\n",
       "  ('E', 'U'): 0.0,\n",
       "  ('E', 'V'): 0.0,\n",
       "  ('E', 'X'): 0.0,\n",
       "  ('E', 'Y'): 0.0,\n",
       "  ('E', 'Z'): 0.0,\n",
       "  ('E', '^'): 0.0,\n",
       "  ('G', '!'): 0.0,\n",
       "  ('G', '#'): 0.0,\n",
       "  ('G', '$'): 0.0,\n",
       "  ('G', '&'): 0.0,\n",
       "  ('G', ','): 0.0,\n",
       "  ('G', '@'): 0.0,\n",
       "  ('G', 'A'): 0.0,\n",
       "  ('G', 'D'): 0.0,\n",
       "  ('G', 'E'): 0.0,\n",
       "  ('G', 'G'): 0.0,\n",
       "  ('G', 'L'): 0.0,\n",
       "  ('G', 'M'): 0.0,\n",
       "  ('G', 'N'): 0.0,\n",
       "  ('G', 'O'): 0.0,\n",
       "  ('G', 'P'): 0.0,\n",
       "  ('G', 'R'): 0.0,\n",
       "  ('G', 'S'): 0.0,\n",
       "  ('G', 'T'): 0.0,\n",
       "  ('G', 'U'): 0.0,\n",
       "  ('G', 'V'): 0.0,\n",
       "  ('G', 'X'): 0.0,\n",
       "  ('G', 'Y'): 0.0,\n",
       "  ('G', 'Z'): 0.0,\n",
       "  ('G', '^'): 0.0,\n",
       "  ('L', '!'): 0.0,\n",
       "  ('L', '#'): 0.0,\n",
       "  ('L', '$'): 0.0,\n",
       "  ('L', '&'): 0.0,\n",
       "  ('L', ','): 0.0,\n",
       "  ('L', '@'): 0.0,\n",
       "  ('L', 'A'): 0.0,\n",
       "  ('L', 'D'): 0.0,\n",
       "  ('L', 'E'): 0.0,\n",
       "  ('L', 'G'): 0.0,\n",
       "  ('L', 'L'): 0.0,\n",
       "  ('L', 'M'): 0.0,\n",
       "  ('L', 'N'): 0.0,\n",
       "  ('L', 'O'): 0.0,\n",
       "  ('L', 'P'): 0.0,\n",
       "  ('L', 'R'): 0.0,\n",
       "  ('L', 'S'): 0.0,\n",
       "  ('L', 'T'): 0.0,\n",
       "  ('L', 'U'): 0.0,\n",
       "  ('L', 'V'): 0.0,\n",
       "  ('L', 'X'): 0.0,\n",
       "  ('L', 'Y'): 0.0,\n",
       "  ('L', 'Z'): 0.0,\n",
       "  ('L', '^'): 0.0,\n",
       "  ('M', '!'): 0.0,\n",
       "  ('M', '#'): 0.0,\n",
       "  ('M', '$'): 0.0,\n",
       "  ('M', '&'): 0.0,\n",
       "  ('M', ','): 0.0,\n",
       "  ('M', '@'): 0.0,\n",
       "  ('M', 'A'): 0.0,\n",
       "  ('M', 'D'): 0.0,\n",
       "  ('M', 'E'): 0.0,\n",
       "  ('M', 'G'): 0.0,\n",
       "  ('M', 'L'): 0.0,\n",
       "  ('M', 'M'): 0.0,\n",
       "  ('M', 'N'): 0.0,\n",
       "  ('M', 'O'): 0.0,\n",
       "  ('M', 'P'): 0.0,\n",
       "  ('M', 'R'): 0.0,\n",
       "  ('M', 'S'): 0.0,\n",
       "  ('M', 'T'): 0.0,\n",
       "  ('M', 'U'): 0.0,\n",
       "  ('M', 'V'): 0.0,\n",
       "  ('M', 'X'): 0.0,\n",
       "  ('M', 'Y'): 0.0,\n",
       "  ('M', 'Z'): 0.0,\n",
       "  ('M', '^'): 0.0,\n",
       "  ('N', '!'): 0.0,\n",
       "  ('N', '#'): 0.0,\n",
       "  ('N', '$'): 0.0,\n",
       "  ('N', '&'): 0.0,\n",
       "  ('N', ','): 0.0,\n",
       "  ('N', '@'): 0.0,\n",
       "  ('N', 'A'): 0.0,\n",
       "  ('N', 'D'): 0.0,\n",
       "  ('N', 'E'): 0.0,\n",
       "  ('N', 'G'): 0.0,\n",
       "  ('N', 'L'): 0.0,\n",
       "  ('N', 'M'): 0.0,\n",
       "  ('N', 'N'): 0.0,\n",
       "  ('N', 'O'): 0.0,\n",
       "  ('N', 'P'): 0.0,\n",
       "  ('N', 'R'): 0.0,\n",
       "  ('N', 'S'): 0.0,\n",
       "  ('N', 'T'): 0.0,\n",
       "  ('N', 'U'): 0.0,\n",
       "  ('N', 'V'): 0.0,\n",
       "  ('N', 'X'): 0.0,\n",
       "  ('N', 'Y'): 0.0,\n",
       "  ('N', 'Z'): 0.0,\n",
       "  ('N', '^'): 0.0,\n",
       "  ('O', '!'): 0.0,\n",
       "  ('O', '#'): 0.0,\n",
       "  ('O', '$'): 0.0,\n",
       "  ('O', '&'): 0.0,\n",
       "  ('O', ','): 0.0,\n",
       "  ('O', '@'): 0.0,\n",
       "  ('O', 'A'): 0.0,\n",
       "  ('O', 'D'): 0.0,\n",
       "  ('O', 'E'): 0.0,\n",
       "  ('O', 'G'): 0.0,\n",
       "  ('O', 'L'): 0.0,\n",
       "  ('O', 'M'): 0.0,\n",
       "  ('O', 'N'): 0.0,\n",
       "  ('O', 'O'): 0.0,\n",
       "  ('O', 'P'): 0.0,\n",
       "  ('O', 'R'): 0.0,\n",
       "  ('O', 'S'): 0.0,\n",
       "  ('O', 'T'): 0.0,\n",
       "  ('O', 'U'): 0.0,\n",
       "  ('O', 'V'): 0.0,\n",
       "  ('O', 'X'): 0.0,\n",
       "  ('O', 'Y'): 0.0,\n",
       "  ('O', 'Z'): 0.0,\n",
       "  ('O', '^'): 0.0,\n",
       "  ('P', '!'): 0.0,\n",
       "  ('P', '#'): 0.0,\n",
       "  ('P', '$'): 0.0,\n",
       "  ('P', '&'): 0.0,\n",
       "  ('P', ','): 0.0,\n",
       "  ('P', '@'): 0.0,\n",
       "  ('P', 'A'): 0.0,\n",
       "  ('P', 'D'): 0.0,\n",
       "  ('P', 'E'): 0.0,\n",
       "  ('P', 'G'): 0.0,\n",
       "  ('P', 'L'): 0.0,\n",
       "  ('P', 'M'): 0.0,\n",
       "  ('P', 'N'): 0.0,\n",
       "  ('P', 'O'): 0.0,\n",
       "  ('P', 'P'): 0.0,\n",
       "  ('P', 'R'): 0.0,\n",
       "  ('P', 'S'): 0.0,\n",
       "  ('P', 'T'): 0.0,\n",
       "  ('P', 'U'): 0.0,\n",
       "  ('P', 'V'): 0.0,\n",
       "  ('P', 'X'): 0.0,\n",
       "  ('P', 'Y'): 0.0,\n",
       "  ('P', 'Z'): 0.0,\n",
       "  ('P', '^'): 0.0,\n",
       "  ('R', '!'): 0.0,\n",
       "  ('R', '#'): 0.0,\n",
       "  ('R', '$'): 0.0,\n",
       "  ('R', '&'): 0.0,\n",
       "  ('R', ','): 0.0,\n",
       "  ('R', '@'): 0.0,\n",
       "  ('R', 'A'): 0.0,\n",
       "  ('R', 'D'): 0.0,\n",
       "  ('R', 'E'): 0.0,\n",
       "  ('R', 'G'): 0.0,\n",
       "  ('R', 'L'): 0.0,\n",
       "  ('R', 'M'): 0.0,\n",
       "  ('R', 'N'): 0.0,\n",
       "  ('R', 'O'): 0.0,\n",
       "  ('R', 'P'): 0.0,\n",
       "  ('R', 'R'): 0.0,\n",
       "  ('R', 'S'): 0.0,\n",
       "  ('R', 'T'): 0.0,\n",
       "  ('R', 'U'): 0.0,\n",
       "  ('R', 'V'): 0.0,\n",
       "  ('R', 'X'): 0.0,\n",
       "  ('R', 'Y'): 0.0,\n",
       "  ('R', 'Z'): 0.0,\n",
       "  ('R', '^'): 0.0,\n",
       "  ('S', '!'): 0.0,\n",
       "  ('S', '#'): 0.0,\n",
       "  ('S', '$'): 0.0,\n",
       "  ('S', '&'): 0.0,\n",
       "  ('S', ','): 0.0,\n",
       "  ('S', '@'): 0.0,\n",
       "  ('S', 'A'): 0.0,\n",
       "  ('S', 'D'): 0.0,\n",
       "  ('S', 'E'): 0.0,\n",
       "  ('S', 'G'): 0.0,\n",
       "  ('S', 'L'): 0.0,\n",
       "  ('S', 'M'): 0.0,\n",
       "  ('S', 'N'): 0.0,\n",
       "  ('S', 'O'): 0.0,\n",
       "  ('S', 'P'): 0.0,\n",
       "  ('S', 'R'): 0.0,\n",
       "  ('S', 'S'): 0.0,\n",
       "  ('S', 'T'): 0.0,\n",
       "  ('S', 'U'): 0.0,\n",
       "  ('S', 'V'): 0.0,\n",
       "  ('S', 'X'): 0.0,\n",
       "  ('S', 'Y'): 0.0,\n",
       "  ('S', 'Z'): 0.0,\n",
       "  ('S', '^'): 0.0,\n",
       "  ('T', '!'): 0.0,\n",
       "  ('T', '#'): 0.0,\n",
       "  ('T', '$'): 0.0,\n",
       "  ('T', '&'): 0.0,\n",
       "  ('T', ','): 0.0,\n",
       "  ('T', '@'): 0.0,\n",
       "  ('T', 'A'): 0.0,\n",
       "  ('T', 'D'): 0.0,\n",
       "  ('T', 'E'): 0.0,\n",
       "  ('T', 'G'): 0.0,\n",
       "  ('T', 'L'): 0.0,\n",
       "  ('T', 'M'): 0.0,\n",
       "  ('T', 'N'): 0.0,\n",
       "  ('T', 'O'): 0.0,\n",
       "  ('T', 'P'): 0.0,\n",
       "  ('T', 'R'): 0.0,\n",
       "  ('T', 'S'): 0.0,\n",
       "  ('T', 'T'): 0.0,\n",
       "  ('T', 'U'): 0.0,\n",
       "  ('T', 'V'): 0.0,\n",
       "  ('T', 'X'): 0.0,\n",
       "  ('T', 'Y'): 0.0,\n",
       "  ('T', 'Z'): 0.0,\n",
       "  ('T', '^'): 0.0,\n",
       "  ('U', '!'): 0.0,\n",
       "  ('U', '#'): 0.0,\n",
       "  ('U', '$'): 0.0,\n",
       "  ('U', '&'): 0.0,\n",
       "  ('U', ','): 0.0,\n",
       "  ('U', '@'): 0.0,\n",
       "  ('U', 'A'): 0.0,\n",
       "  ('U', 'D'): 0.0,\n",
       "  ('U', 'E'): 0.0,\n",
       "  ('U', 'G'): 0.0,\n",
       "  ('U', 'L'): 0.0,\n",
       "  ('U', 'M'): 0.0,\n",
       "  ('U', 'N'): 0.0,\n",
       "  ('U', 'O'): 0.0,\n",
       "  ('U', 'P'): 0.0,\n",
       "  ('U', 'R'): 0.0,\n",
       "  ('U', 'S'): 0.0,\n",
       "  ('U', 'T'): 0.0,\n",
       "  ('U', 'U'): 0.0,\n",
       "  ('U', 'V'): 0.0,\n",
       "  ('U', 'X'): 0.0,\n",
       "  ('U', 'Y'): 0.0,\n",
       "  ('U', 'Z'): 0.0,\n",
       "  ('U', '^'): 0.0,\n",
       "  ('V', '!'): 0.0,\n",
       "  ('V', '#'): 0.0,\n",
       "  ('V', '$'): 0.0,\n",
       "  ('V', '&'): 0.0,\n",
       "  ('V', ','): 0.0,\n",
       "  ('V', '@'): 0.0,\n",
       "  ('V', 'A'): 0.0,\n",
       "  ('V', 'D'): 0.0,\n",
       "  ('V', 'E'): 0.0,\n",
       "  ('V', 'G'): 0.0,\n",
       "  ('V', 'L'): 0.0,\n",
       "  ('V', 'M'): 0.0,\n",
       "  ('V', 'N'): 0.0,\n",
       "  ('V', 'O'): 0.0,\n",
       "  ('V', 'P'): 0.0,\n",
       "  ('V', 'R'): 0.0,\n",
       "  ('V', 'S'): 0.0,\n",
       "  ('V', 'T'): 0.0,\n",
       "  ('V', 'U'): 0.0,\n",
       "  ('V', 'V'): 0.0,\n",
       "  ('V', 'X'): 0.0,\n",
       "  ('V', 'Y'): 0.0,\n",
       "  ('V', 'Z'): 0.0,\n",
       "  ('V', '^'): 0.0,\n",
       "  ('X', '!'): 0.0,\n",
       "  ('X', '#'): 0.0,\n",
       "  ('X', '$'): 0.0,\n",
       "  ('X', '&'): 0.0,\n",
       "  ('X', ','): 0.0,\n",
       "  ('X', '@'): 0.0,\n",
       "  ('X', 'A'): 0.0,\n",
       "  ('X', 'D'): 0.0,\n",
       "  ('X', 'E'): 0.0,\n",
       "  ('X', 'G'): 0.0,\n",
       "  ('X', 'L'): 0.0,\n",
       "  ('X', 'M'): 0.0,\n",
       "  ('X', 'N'): 0.0,\n",
       "  ('X', 'O'): 0.0,\n",
       "  ('X', 'P'): 0.0,\n",
       "  ('X', 'R'): 0.0,\n",
       "  ('X', 'S'): 0.0,\n",
       "  ('X', 'T'): 0.0,\n",
       "  ('X', 'U'): 0.0,\n",
       "  ('X', 'V'): 0.0,\n",
       "  ('X', 'X'): 0.0,\n",
       "  ('X', 'Y'): 0.0,\n",
       "  ('X', 'Z'): 0.0,\n",
       "  ('X', '^'): 0.0,\n",
       "  ('Y', '!'): 0.0,\n",
       "  ('Y', '#'): 0.0,\n",
       "  ('Y', '$'): 0.0,\n",
       "  ('Y', '&'): 0.0,\n",
       "  ('Y', ','): 0.0,\n",
       "  ('Y', '@'): 0.0,\n",
       "  ('Y', 'A'): 0.0,\n",
       "  ('Y', 'D'): 0.0,\n",
       "  ('Y', 'E'): 0.0,\n",
       "  ('Y', 'G'): 0.0,\n",
       "  ('Y', 'L'): 0.0,\n",
       "  ('Y', 'M'): 0.0,\n",
       "  ('Y', 'N'): 0.0,\n",
       "  ('Y', 'O'): 0.0,\n",
       "  ('Y', 'P'): 0.0,\n",
       "  ('Y', 'R'): 0.0,\n",
       "  ('Y', 'S'): 0.0,\n",
       "  ('Y', 'T'): 0.0,\n",
       "  ('Y', 'U'): 0.0,\n",
       "  ('Y', 'V'): 0.0,\n",
       "  ('Y', 'X'): 0.0,\n",
       "  ('Y', 'Y'): 0.0,\n",
       "  ('Y', 'Z'): 0.0,\n",
       "  ('Y', '^'): 0.0,\n",
       "  ('Z', '!'): 0.0,\n",
       "  ('Z', '#'): 0.0,\n",
       "  ('Z', '$'): 0.0,\n",
       "  ('Z', '&'): 0.0,\n",
       "  ('Z', ','): 0.0,\n",
       "  ('Z', '@'): 0.0,\n",
       "  ('Z', 'A'): 0.0,\n",
       "  ('Z', 'D'): 0.0,\n",
       "  ('Z', 'E'): 0.0,\n",
       "  ('Z', 'G'): 0.0,\n",
       "  ('Z', 'L'): 0.0,\n",
       "  ('Z', 'M'): 0.0,\n",
       "  ('Z', 'N'): 0.0,\n",
       "  ('Z', 'O'): 0.0,\n",
       "  ('Z', 'P'): 0.0,\n",
       "  ('Z', 'R'): 0.0,\n",
       "  ('Z', 'S'): 0.0,\n",
       "  ('Z', 'T'): 0.0,\n",
       "  ('Z', 'U'): 0.0,\n",
       "  ('Z', 'V'): 0.0,\n",
       "  ('Z', 'X'): 0.0,\n",
       "  ('Z', 'Y'): 0.0,\n",
       "  ('Z', 'Z'): 0.0,\n",
       "  ('Z', '^'): 0.0,\n",
       "  ('^', '!'): 0.0,\n",
       "  ('^', '#'): 0.0,\n",
       "  ('^', '$'): 0.0,\n",
       "  ('^', '&'): 0.0,\n",
       "  ('^', ','): 0.0,\n",
       "  ('^', '@'): 0.0,\n",
       "  ('^', 'A'): 0.0,\n",
       "  ('^', 'D'): 0.0,\n",
       "  ('^', 'E'): 0.0,\n",
       "  ('^', 'G'): 0.0,\n",
       "  ('^', 'L'): 0.0,\n",
       "  ('^', 'M'): 0.0,\n",
       "  ('^', 'N'): 0.0,\n",
       "  ('^', 'O'): 0.0,\n",
       "  ('^', 'P'): 0.0,\n",
       "  ('^', 'R'): 0.0,\n",
       "  ('^', 'S'): 0.0,\n",
       "  ('^', 'T'): 0.0,\n",
       "  ('^', 'U'): 0.0,\n",
       "  ('^', 'V'): 0.0,\n",
       "  ('^', 'X'): 0.0,\n",
       "  ('^', 'Y'): 0.0,\n",
       "  ('^', 'Z'): 0.0,\n",
       "  ('^', '^'): 0.0},\n",
       " [defaultdict(float,\n",
       "              {'!': 0,\n",
       "               '#': 0,\n",
       "               '$': 0,\n",
       "               '&': 0,\n",
       "               ',': 0,\n",
       "               '@': 0,\n",
       "               'A': 0,\n",
       "               'D': 0,\n",
       "               'E': 0,\n",
       "               'G': 0,\n",
       "               'L': 0,\n",
       "               'M': 0,\n",
       "               'N': 10.0,\n",
       "               'O': 0,\n",
       "               'P': 0,\n",
       "               'R': 0,\n",
       "               'S': 0,\n",
       "               'T': 0,\n",
       "               'U': 0,\n",
       "               'V': 0,\n",
       "               'X': 0,\n",
       "               'Y': 0,\n",
       "               'Z': 0,\n",
       "               '^': 0}),\n",
       "  defaultdict(float,\n",
       "              {'!': 0,\n",
       "               '#': 0,\n",
       "               '$': 0,\n",
       "               '&': 6.0,\n",
       "               ',': 0,\n",
       "               '@': 0,\n",
       "               'A': 0,\n",
       "               'D': 0,\n",
       "               'E': 0,\n",
       "               'G': 0,\n",
       "               'L': 0,\n",
       "               'M': 0,\n",
       "               'N': 0,\n",
       "               'O': 0,\n",
       "               'P': 0,\n",
       "               'R': 0,\n",
       "               'S': 0,\n",
       "               'T': 0,\n",
       "               'U': 0,\n",
       "               'V': 0,\n",
       "               'X': 0,\n",
       "               'Y': 0,\n",
       "               'Z': 0,\n",
       "               '^': 0}),\n",
       "  defaultdict(float,\n",
       "              {'!': 0,\n",
       "               '#': 0,\n",
       "               '$': 0,\n",
       "               '&': 0,\n",
       "               ',': 0,\n",
       "               '@': 0,\n",
       "               'A': 0,\n",
       "               'D': 0,\n",
       "               'E': 0,\n",
       "               'G': 0,\n",
       "               'L': 0,\n",
       "               'M': 0,\n",
       "               'N': 7.0,\n",
       "               'O': 0,\n",
       "               'P': 0,\n",
       "               'R': 0,\n",
       "               'S': 0,\n",
       "               'T': 0,\n",
       "               'U': 0,\n",
       "               'V': 0,\n",
       "               'X': 0,\n",
       "               'Y': 0,\n",
       "               'Z': 0,\n",
       "               '^': 0})])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [\"More\",\"than\",\"feeling\"]\n",
    "weights = defaultdict(float)\n",
    "weights['N','&'] = 5\n",
    "weights['&','N'] = 5\n",
    "weights['tag=N_curword=More'] = 10.0\n",
    "weights['tag=N_curword=feeling'] = 7.0\n",
    "weights['tag=&_curword=than'] = 6.0\n",
    "calc_factor_scores(tokens,weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.6** (4 points)\n",
    "\n",
    "Implement `predict_seq()`, which predicts the tags for an input sentence, given a model. It will have to calculate the factor scores, then call Viterbi as a subroutine, then return the best sequence prediction. If your Viterbi implementation does not seem to be working, use the implementation of the greedy decoding\n",
    "algorithm that we provide (it uses the same inputs as `vit_starter.viterbi()`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N', '&', 'N']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [\"More\",\"than\",\"feeling\"]\n",
    "weights = defaultdict(float)\n",
    "weights['N','A'] = 5\n",
    "weights['A','N'] = 5\n",
    "weights['tag=N_curword=More'] = 10.0\n",
    "weights['tag=N_curword=feeling'] = 7.0\n",
    "weights['tag=&_curword=than'] = 6.0\n",
    "predict_seq(tokens, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, you're done with the inference part.  Time to put it all together into the parameter learning algorithm and see it go. \n",
    "\n",
    "**Question 5.7** (14 points)\n",
    "\n",
    "Implement `train()`, which does structured perceptron training with the averaged perceptron algorithm. You should train on oct27.train, and evaluate on oct27.dev. You will want to first get it working without averaging, then add averaging to it. Run it for 10 iterations, and print the devset accuracy at each training iteration. Note that we provide evaluation code, which assumes `predict_seq()` and everything it depends on is working properly.\n",
    "\n",
    "For us, here's the performance we get at the first and last iterations, using the features in the starter code (just the bias term and the current word feature, without case normalization).\n",
    "\n",
    "`\n",
    "Training iteration 0\n",
    "DEV RAW EVAL: 2556/4823 = 0.5300 accuracy\n",
    "DEV AVG EVAL: 2986/4823 = 0.6191 accuracy\n",
    "...\n",
    "Training iteration 9\n",
    "DEV RAW EVAL: 3232/4823 = 0.6701 accuracy\n",
    "DEV AVG EVAL: 3341/4823 = 0.6927 accuracy\n",
    "Learned weights for 24361 features from 1000 examples\n",
    "`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[training...]\n",
      "\tTraining iteration 0\n",
      "DEV RAW EVAL: 1161/2000 = 0.4305 accuracy\n",
      "DEV AVG EVAL: 1187/2000 = 0.4435 accuracy\n",
      "\tTraining iteration 1\n",
      "DEV RAW EVAL: 1475/2000 = 0.5875 accuracy\n",
      "DEV AVG EVAL: 1482/2000 = 0.5910 accuracy\n",
      "\tTraining iteration 2\n",
      "DEV RAW EVAL: 1439/2000 = 0.5695 accuracy\n",
      "DEV AVG EVAL: 1467/2000 = 0.5835 accuracy\n",
      "\tTraining iteration 3\n",
      "DEV RAW EVAL: 1558/2000 = 0.6290 accuracy\n",
      "DEV AVG EVAL: 1566/2000 = 0.6330 accuracy\n",
      "\tTraining iteration 4\n",
      "DEV RAW EVAL: 1547/2000 = 0.6235 accuracy\n",
      "DEV AVG EVAL: 1540/2000 = 0.6200 accuracy\n",
      "\tTraining iteration 5\n",
      "DEV RAW EVAL: 1450/2000 = 0.5750 accuracy\n",
      "DEV AVG EVAL: 1456/2000 = 0.5780 accuracy\n",
      "\tTraining iteration 6\n",
      "DEV RAW EVAL: 1591/2000 = 0.6455 accuracy\n",
      "DEV AVG EVAL: 1606/2000 = 0.6530 accuracy\n",
      "\tTraining iteration 7\n",
      "DEV RAW EVAL: 1554/2000 = 0.6270 accuracy\n",
      "DEV AVG EVAL: 1570/2000 = 0.6350 accuracy\n",
      "\tTraining iteration 8\n",
      "DEV RAW EVAL: 1610/2000 = 0.6550 accuracy\n",
      "DEV AVG EVAL: 1627/2000 = 0.6635 accuracy\n",
      "\tTraining iteration 9\n",
      "DEV RAW EVAL: 1557/2000 = 0.6285 accuracy\n",
      "DEV AVG EVAL: 1571/2000 = 0.6355 accuracy\n",
      "[learned weights for 77254 features from 2000 examples.]\n"
     ]
    }
   ],
   "source": [
    "y=f1.trains(dtr, stepsize=1, numpasses=10, do_averaging=True, devdata=dte)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.8** (6 points)\n",
    "\n",
    "Print out a report of the accuracy rate for each tag in the development set. We provided a function to do this `fancy_eval`. Look at the two sentences in the dev data, and in your writeup show and compare the gold-standard tags versus your model's predictions for them.  Consult the tagset description to understand what's going on.\n",
    "What types of things does your tagger get right and wrong?\n",
    "\n",
    "To look at the examples, you may find it convenient to use `show_predictions` (or write up the equivalent manually).  For example, after 1 iteration of training, we get this output from the first sentence in the devset.\n",
    "(After investigating TV shows that were popular in 2011 when the tweet was authored, we actually think some of the gold-standard tags in this example might be wrong.)\n",
    "\n",
    "\n",
    "              word                 gold pred\n",
    "              ----                 ---- ----\n",
    "              @ciaranyree          @    @   \n",
    "              it                   O    O   \n",
    "              was                  V    V   \n",
    "              on                   P    P   \n",
    "              football             N    ^     *** Error\n",
    "              wives                N    N   \n",
    "              ,                    ,    ,   \n",
    "              one                  $    $   \n",
    "              of                   P    P   \n",
    "              the                  D    D   \n",
    "              players              N    N   \n",
    "              and                  &    &   \n",
    "              his                  D    D   \n",
    "              wife                 N    N   \n",
    "              own                  V    V   \n",
    "              smash                ^    D     *** Error\n",
    "              burger               ^    N     *** Error\n",
    "\n",
    "\n",
    "To do this part, you may find it useful to save your model's weights with pickle.dumps (or json.dumps) and have a short analysis script that loads the model and devdata to do the reports.  If you have to re-train each time you tweak your analysis code, it can be annoying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.9** (OPTIONAL: 4 Extra Credit points)\n",
    "\n",
    "Improve the features of your tagger to improve accuracy on the development set. This will only require changes to `local_emission_features`. Implement at least 4 new types of features. Report your tagger's accuracy with these improvements. Please make a table that reports accuracy from adding different features. The first row should be the basic system, and the last row should be the fanciest system. Rows in between should report different combinations of features. One simple way to do this is, if you have 4 different feature types, to run 4 experiments where in each one, you add only one feature type to the basic system. For example:\n",
    "\n",
    "<img src=\"3.png\"> \n",
    "\n",
    "Hint: if you make features about the first character of a word, that helps a lot for the # (hashtag) and @ (at-mention) tags.  The URL tag is easy to get too with a similar form of character affix analysis. Character affixes help lots of other tags too. Also, if you have a feature that looks at the word at position $t$, you can make new versions of it that look to the left or right of the $t^{th}$ position in question: for example, 'word_to_left=the'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
